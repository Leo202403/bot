#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã€V8.3.21ã€‘å›æµ‹ä¼˜åŒ–æ¨¡å— - è½»é‡çº§ã€æˆæœ¬ä¼˜åŒ–ã€èµ„æºæ§åˆ¶

ç‰¹æ€§ï¼š
1. å¤šç»´åº¦Grid Searchï¼ˆ11ä¸ªå‚æ•°ç»´åº¦ï¼‰
2. V8.3.21ä¸Šä¸‹æ–‡è¿‡æ»¤ï¼ˆKçº¿/å¸‚åœºç»“æ„/S/Rå†å²ï¼‰
3. æœ¬åœ°ç»Ÿè®¡åˆ†æï¼ˆå‚æ•°æ•æ„Ÿåº¦ã€å¼‚å¸¸æ£€æµ‹ï¼‰
4. æˆæœ¬ä¼˜åŒ–çš„AIå†³ç­–ï¼ˆå‹ç¼©æ•°æ®ã€ç²¾ç®€Promptï¼‰
5. èµ„æºæ§åˆ¶ï¼ˆé™åˆ¶å†…å­˜ã€CPU niceå€¼ã€è¿›ç¨‹éš”ç¦»ï¼‰

é€‚ç”¨ç¯å¢ƒï¼š2æ ¸2GæœåŠ¡å™¨
"""

import os
import gc
import random
import numpy as np
from typing import Dict, List, Tuple, Any
from datetime import datetime

# å°è¯•å¯¼å…¥psutilï¼ˆå¯é€‰ï¼‰
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False


# ============================================================
# ã€æ­¥éª¤2ã€‘è½»é‡çº§Grid Searchï¼ˆèµ„æºæ§åˆ¶ï¼‰
# ============================================================

def optimize_params_v8321_lightweight(opportunities: List[Dict], 
                                      current_params: Dict, 
                                      signal_type: str = 'scalping',
                                      max_combinations: int = 200,
                                      ai_suggested_params: Dict = None) -> Dict:
    """
    ã€V8.3.21ã€‘è½»é‡çº§å‚æ•°ä¼˜åŒ–
    
    è®¾è®¡ï¼š
    - 2æ ¸CPUï¼šä½¿ç”¨éšæœºé‡‡æ ·ä»£æ›¿éå†ï¼ˆ200ç»„ vs 2592ç»„ï¼‰
    - 2Gå†…å­˜ï¼šåŠæ—¶é‡Šæ”¾å†…å­˜ï¼Œæ¯10ç»„GCä¸€æ¬¡
    - è¿›ç¨‹éš”ç¦»ï¼šè®¾ç½®niceå€¼ï¼Œé¿å…å½±å“å®æ—¶AI
    
    Args:
        opportunities: æœºä¼šåˆ—è¡¨ï¼ˆå·²åŒ…å«V8.3.21å­—æ®µï¼‰
        current_params: å½“å‰å‚æ•°
        signal_type: 'scalping' or 'swing'
        max_combinations: æœ€å¤§æµ‹è¯•ç»„æ•°ï¼ˆé»˜è®¤200ï¼‰
        ai_suggested_params: ã€V8.3.25.10æ–°å¢ã€‘AIæ´å¯Ÿå»ºè®®çš„å‚æ•°ï¼ˆå°†åŠ å…¥æµ‹è¯•å€™é€‰é›†ï¼‰
    
    Returns:
        {
            'optimized_params': {...},
            'top_10_configs': [...],
            'statistics': {...},
            'cost_saved': 0.xx
        }
    """
    # è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§ï¼ˆniceå€¼=10ï¼Œé¿å…å½±å“å®æ—¶AIï¼‰
    try:
        os.nice(10)
        print(f"   â„¹ï¸  å·²è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§ï¼ˆnice=10ï¼‰ï¼Œé¿å…å½±å“å®æ—¶AI")
    except:
        pass
    
    print(f"\n{'='*60}")
    print(f"ã€V8.3.21å›æµ‹ä¼˜åŒ–ã€‘è½»é‡çº§å‚æ•°æœç´¢ï¼ˆ{signal_type}ï¼‰")
    print(f"  æœºä¼šæ•°: {len(opportunities)}")
    print(f"  æµ‹è¯•ç»„æ•°: {max_combinations}")
    if HAS_PSUTIL:
        print(f"  å†…å­˜é™åˆ¶: æ£€æµ‹åˆ°{psutil.virtual_memory().total / (1024**3):.1f}Gï¼Œå°†ä¸»åŠ¨æ§åˆ¶")
    else:
        print(f"  èµ„æºç›‘æ§: ä¸å¯ç”¨ï¼ˆpsutilæœªå®‰è£…ï¼‰")
    print(f"{'='*60}\n")
    
    # ===== é˜¶æ®µ1ï¼šå®šä¹‰æœç´¢ç©ºé—´ =====
    print("ğŸ“Š é˜¶æ®µ1: å®šä¹‰æœç´¢ç©ºé—´...")
    
    # ã€V8.4.4ã€‘ä¼ å…¥current_paramsä½œä¸ºbaselineï¼Œå…è®¸åŠ¨æ€è°ƒæ•´æœç´¢ä¸­å¿ƒ
    param_grid = define_param_grid_v8321(signal_type, baseline_params=current_params)
    total_combinations = calculate_total_combinations(param_grid)
    
    print(f"   âœ… æœç´¢ç©ºé—´å®šä¹‰å®Œæˆ")
    print(f"      ç†è®ºç»„åˆæ•°: {total_combinations}ç»„")
    print(f"      å®é™…æµ‹è¯•æ•°: {max_combinations}ç»„ï¼ˆéšæœºé‡‡æ ·ï¼‰")
    
    # ===== é˜¶æ®µ2ï¼šéšæœºé‡‡æ ·Grid Search =====
    print(f"\nğŸ” é˜¶æ®µ2: éšæœºé‡‡æ ·Grid Search...")
    
    sampled_params = random_sample_param_grid(param_grid, max_combinations)
    
    # ã€V8.3.25.10ã€‘å°†AIå»ºè®®çš„å‚æ•°åŠ å…¥æµ‹è¯•å€™é€‰é›†
    if ai_suggested_params:
        print(f"   ğŸ¤– å‘ç°AIå»ºè®®å‚æ•°ï¼ŒåŠ å…¥æµ‹è¯•å€™é€‰é›†...")
        ai_config = {}
        # ğŸ”§ V8.3.25.12: åªä¿ç•™æœç´¢ç©ºé—´ä¸­å­˜åœ¨çš„å‚æ•°
        valid_param_names = set(param_grid.keys())
        for key, value in ai_suggested_params.items():
            if key in valid_param_names:
                ai_config[key] = value
            else:
                print(f"      âš ï¸  è·³è¿‡ä¸åœ¨æœç´¢ç©ºé—´ä¸­çš„å‚æ•°: {key}={value}")
        
        if ai_config:
            # ç¡®ä¿AIå»ºè®®çš„å‚æ•°åœ¨å€™é€‰é›†çš„å‰åˆ—ï¼ˆä¼˜å…ˆæµ‹è¯•ï¼‰
            sampled_params.insert(0, ai_config)
            print(f"      âœ… AIå»ºè®®å‚æ•°å·²åŠ å…¥ï¼ˆä¼˜å…ˆæµ‹è¯•ï¼‰: {ai_config}")
        else:
            print(f"      â„¹ï¸  AIå»ºè®®çš„å‚æ•°éƒ½ä¸åœ¨æœç´¢ç©ºé—´ä¸­ï¼Œè·³è¿‡")
    
    all_results = []
    
    for i, params in enumerate(sampled_params):
        # å†…å­˜æ£€æŸ¥ï¼ˆæ¯10ç»„æ£€æŸ¥ä¸€æ¬¡ï¼‰
        if i % 10 == 0 and HAS_PSUTIL:
            mem_usage = psutil.Process().memory_info().rss / (1024**2)
            if mem_usage > 300:  # è¶…è¿‡300MBåˆ™GC
                gc.collect()
                print(f"      [{i}/{max_combinations}] å†…å­˜: {mem_usage:.0f}MB â†’ GC")
        
        # æ¨¡æ‹Ÿè¿™ä¸ªå‚æ•°é…ç½®
        result = simulate_params_with_v8321_filter(opportunities, params)
        score = calculate_v8321_optimization_score(result)
        
        all_results.append({
            'params': params,
            'score': score,
            'metrics': extract_key_metrics(result)
        })
        
        # è¿›åº¦æ˜¾ç¤º
        if (i + 1) % 20 == 0:
            print(f"      è¿›åº¦: {i+1}/{max_combinations}...")
    
    # æ’åºå¹¶å–Top 10
    top_10 = sorted(all_results, key=lambda x: x['score'], reverse=True)[:10]
    
    print(f"   âœ… Grid Searchå®Œæˆ")
    print(f"      æœ€é«˜åˆ†: {top_10[0]['score']:.3f}")
    print(f"      æµ‹è¯•ç»„æ•°: {len(all_results)}")
    
    # ä¸»åŠ¨GC
    gc.collect()
    
    # ===== é˜¶æ®µ3ï¼šæœ¬åœ°ç»Ÿè®¡åˆ†æ =====
    print(f"\nğŸ“ˆ é˜¶æ®µ3: æœ¬åœ°ç»Ÿè®¡åˆ†æï¼ˆå…è´¹ï¼‰...")
    
    # æœ¬åœ°è®¡ç®—ï¼šå‚æ•°æ•æ„Ÿåº¦
    param_sensitivity = calculate_param_sensitivity_local(all_results)
    
    # æœ¬åœ°è®¡ç®—ï¼šä¸Šä¸‹æ–‡ç‰¹å¾ç›¸å…³æ€§
    context_analysis = analyze_context_features_local(opportunities, top_10[0]['params'])
    
    # æœ¬åœ°æ£€æµ‹ï¼šå¼‚å¸¸æƒ…å†µ
    anomalies = detect_anomalies_local(all_results, param_sensitivity)
    
    print(f"   âœ… ç»Ÿè®¡åˆ†æå®Œæˆ")
    print(f"      å…³é”®å‚æ•°: {list(param_sensitivity.keys())[:3]}")
    print(f"      å¼‚å¸¸æ£€æµ‹: {len(anomalies)}ä¸ª")
    
    # ===== é˜¶æ®µ4ï¼šæ•°æ®å‹ç¼© =====
    print(f"\nğŸ—œï¸  é˜¶æ®µ4: æ•°æ®å‹ç¼©ï¼ˆèŠ‚çœAIæˆæœ¬ï¼‰...")
    
    compressed_data = compress_optimization_results(
        top_10=top_10,
        param_sensitivity=param_sensitivity,
        context_analysis=context_analysis,
        anomalies=anomalies
    )
    
    estimated_tokens = estimate_token_count(compressed_data)
    original_tokens = len(all_results) * 100  # å‡è®¾åŸå§‹æ¯ç»„100 tokens
    cost_saved = (original_tokens - estimated_tokens) * 0.00002  # GPT-4ä»·æ ¼
    
    print(f"   âœ… æ•°æ®å‹ç¼©å®Œæˆ")
    print(f"      åŸå§‹: ~{original_tokens} tokens")
    print(f"      å‹ç¼©å: ~{estimated_tokens} tokens")
    print(f"      ğŸ’° é¢„è®¡èŠ‚çœ: ${cost_saved:.4f}")
    
    # ===== é˜¶æ®µ5ï¼šAIè¿­ä»£å†³ç­–ï¼ˆå¯é€‰ï¼‰=====
    ai_decision = None
    ai_adjusted_params = None
    
    if max_combinations >= 100:  # åªæœ‰å¤§è§„æ¨¡æœç´¢æ‰å€¼å¾—AIä»‹å…¥
        print(f"\nğŸ¤– é˜¶æ®µ5: AIè¿­ä»£å†³ç­–...")
        try:
            ai_decision = call_ai_for_iterative_optimization(
                top_10_configs=top_10,
                param_sensitivity=param_sensitivity,
                context_analysis=context_analysis,
                anomalies=anomalies,
                compressed_data=compressed_data,
                signal_type=signal_type
            )
            
            if ai_decision and ai_decision.get('needs_adjustment'):
                print(f"   ğŸ”§ AIå»ºè®®è°ƒæ•´å‚æ•°...")
                ai_adjusted_params = apply_ai_adjustments(
                    base_params=top_10[0]['params'],
                    adjustments=ai_decision['param_adjustments']
                )
                
                # éªŒè¯AIè°ƒæ•´åçš„å‚æ•°
                ai_result = simulate_params_with_v8321_filter(opportunities, ai_adjusted_params)
                ai_score = calculate_v8321_optimization_score(ai_result)
                
                print(f"   ğŸ“Š AIè°ƒæ•´æ•ˆæœ:")
                print(f"      Gridæœ€ä¼˜: {top_10[0]['score']:.3f}")
                print(f"      AIè°ƒæ•´å: {ai_score:.3f} ({ai_score-top_10[0]['score']:+.3f})")
                
                # å¦‚æœAIè°ƒæ•´åæ›´å¥½ï¼Œä½¿ç”¨AIå‚æ•°
                if ai_score > top_10[0]['score']:
                    print(f"   âœ… AIè°ƒæ•´æœ‰æ•ˆï¼Œé‡‡çº³AIå»ºè®®")
                    final_params = ai_adjusted_params
                    final_score = ai_score
                    cost_saved += 0.01  # AIè°ƒç”¨æˆæœ¬çº¦$0.01
                else:
                    print(f"   âš ï¸  AIè°ƒæ•´æ•ˆæœä¸ä½³ï¼Œä¿æŒGridç»“æœ")
                    final_params = top_10[0]['params']
                    final_score = top_10[0]['score']
            else:
                print(f"   âœ… AIè®¤ä¸ºå½“å‰å‚æ•°å·²æ˜¯æœ€ä¼˜")
                final_params = top_10[0]['params']
                final_score = top_10[0]['score']
                
        except Exception as e:
            print(f"   âš ï¸  AIå†³ç­–å¤±è´¥: {e}")
            final_params = top_10[0]['params']
            final_score = top_10[0]['score']
    else:
        final_params = top_10[0]['params']
        final_score = top_10[0]['score']
    
    return {
        'optimized_params': final_params,
        'top_10_configs': top_10,
        'statistics': {
            'param_sensitivity': param_sensitivity,
            'score_distribution': calculate_score_distribution(all_results)
        },
        'context_analysis': context_analysis,
        'anomalies': anomalies,
        'compressed_data': compressed_data,
        'cost_saved': cost_saved,
        'ai_decision': ai_decision,  # AIå†³ç­–ï¼ˆè‹±æ–‡ï¼‰
        'ai_adjusted_params': ai_adjusted_params  # AIè°ƒæ•´åçš„å‚æ•°
    }


def define_param_grid_v8321(signal_type: str, baseline_params: Dict = None) -> Dict:
    """
    ã€V8.4.4ã€‘å®šä¹‰V8.3.21å‚æ•°æœç´¢ç©ºé—´ï¼ˆåŠ¨æ€èŒƒå›´çº¦æŸï¼‰
    
    æ ¸å¿ƒæ€è·¯ï¼š
    1. å›ºå®šåŸºå‡†å‚æ•°ï¼ˆé˜¶æ®µ2ç”¨äºè®¡ç®—actual_profitï¼Œç¡®ä¿å®¢è§‚æ€§ï¼‰
    2. ä¼˜åŒ–å™¨å¯ä»¥åœ¨åŸºå‡†Â±50%èŒƒå›´å†…æœç´¢ï¼ˆè‡ªé€‚åº”å¸‚åœºæ³¢åŠ¨ï¼‰
    3. è®¾ç½®ç»å¯¹è¾¹ç•Œé˜²æ­¢æç«¯å€¼ï¼ˆå¦‚atr_tp=6.0ï¼‰
    
    ç¤ºä¾‹ï¼ˆæ³¢æ®µï¼‰ï¼š
    - åŸºå‡†ï¼šatr_tp=3.0
    - æœç´¢èŒƒå›´ï¼š[1.5, 3.0, 4.5]ï¼ˆÂ±50%ï¼‰
    - ç»å¯¹è¾¹ç•Œï¼š[2.0, 5.0]ï¼ˆä¸å…è®¸<2.0æˆ–>5.0ï¼‰
    - å®é™…æœç´¢ï¼š[2.0, 3.0, 4.5]
    
    Args:
        signal_type: 'scalping' æˆ– 'swing'
        baseline_params: ä¸Šä¸€æ¬¡ä¼˜åŒ–çš„å‚æ•°ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´æœç´¢ä¸­å¿ƒï¼‰
    
    Returns:
        å‚æ•°æœç´¢ç©ºé—´å­—å…¸
    """
    
    # ã€V8.4.4ã€‘å®šä¹‰å›ºå®šåŸºå‡†å’Œç»å¯¹è¾¹ç•Œ
    if signal_type == 'scalping':
        # è¶…çŸ­çº¿å›ºå®šåŸºå‡†
        baseline = {
            'atr_tp_multiplier': 2.0,
            'atr_stop_multiplier': 1.5,
            'max_holding_hours': 8,
            'min_risk_reward': 1.5
        }
        # ç»å¯¹è¾¹ç•Œï¼ˆç¡¬çº¦æŸï¼Œé˜²æ­¢æç«¯å€¼ï¼‰
        bounds = {
            'atr_tp_multiplier': (1.0, 3.0),      # ä¸å…è®¸>3.0
            'atr_stop_multiplier': (1.0, 2.0),
            'max_holding_hours': (4, 16),
            'min_risk_reward': (0.8, 2.5)
        }
    else:  # swing
        # æ³¢æ®µå›ºå®šåŸºå‡†
        baseline = {
            'atr_tp_multiplier': 3.0,
            'atr_stop_multiplier': 1.5,
            'max_holding_hours': 60,
            'min_risk_reward': 1.5
        }
        # ç»å¯¹è¾¹ç•Œ
        bounds = {
            'atr_tp_multiplier': (2.0, 5.0),      # ä¸å…è®¸>5.0ï¼ˆé˜²æ­¢6.0è¿™æ ·çš„æç«¯å€¼ï¼‰
            'atr_stop_multiplier': (1.0, 2.5),
            'max_holding_hours': (36, 96),
            'min_risk_reward': (0.8, 3.0)
        }
    
    # ã€V8.4.4ã€‘å¦‚æœæä¾›äº†baseline_paramsï¼Œç”¨å®ƒä½œä¸ºæœç´¢ä¸­å¿ƒï¼ˆä½†ä»å—è¾¹ç•Œé™åˆ¶ï¼‰
    if baseline_params:
        for key in ['atr_tp_multiplier', 'atr_stop_multiplier', 'max_holding_hours', 'min_risk_reward']:
            if key in baseline_params:
                value = baseline_params[key]
                min_bound, max_bound = bounds[key]
                # é™åˆ¶åœ¨è¾¹ç•Œå†…
                baseline[key] = max(min_bound, min(max_bound, value))
    
    # ã€V8.4.4ã€‘ç”Ÿæˆæœç´¢ç©ºé—´ï¼ˆåŸºå‡†Â±50%ï¼Œå—ç»å¯¹è¾¹ç•Œé™åˆ¶ï¼‰
    def generate_search_range(param_name, center_value):
        """ç”Ÿæˆæœç´¢èŒƒå›´ï¼šcenter Â± 50%ï¼Œä½†ä¸è¶…è¿‡ç»å¯¹è¾¹ç•Œ"""
        min_bound, max_bound = bounds[param_name]
        
        # è®¡ç®—Â±50%èŒƒå›´
        lower = max(min_bound, center_value * 0.5)
        upper = min(max_bound, center_value * 1.5)
        
        # ç”Ÿæˆ3ä¸ªé‡‡æ ·ç‚¹ï¼šä¸‹é™ã€ä¸­å¿ƒã€ä¸Šé™
        if param_name == 'max_holding_hours':
            # æ•´æ•°å‚æ•°
            return [int(lower), int(center_value), int(upper)]
        else:
            # æµ®ç‚¹å‚æ•°ï¼Œä¿ç•™1ä½å°æ•°
            return [
                round(lower, 1),
                round(center_value, 1),
                round(upper, 1)
            ]
    
    if signal_type == 'scalping':
        grid = {
            # ã€V8.4.4ã€‘åŸºç¡€å‚æ•°ï¼ˆåŠ¨æ€èŒƒå›´ï¼Œå›´ç»•åŸºå‡†Â±50%ï¼‰
            'max_holding_hours': generate_search_range('max_holding_hours', baseline['max_holding_hours']),
            'atr_tp_multiplier': generate_search_range('atr_tp_multiplier', baseline['atr_tp_multiplier']),
            'atr_stop_multiplier': generate_search_range('atr_stop_multiplier', baseline['atr_stop_multiplier']),
            'min_risk_reward': generate_search_range('min_risk_reward', baseline['min_risk_reward']),
            
            # å…¥åœºè¿‡æ»¤å‚æ•°ï¼ˆä¿æŒåŸæœ‰èŒƒå›´ï¼‰
            'min_signal_score': [40, 50, 60],
            'min_consensus_score': [0, 10, 20, 30],
            'min_consensus': [0, 1, 2],
            'min_kline_bullish_ratio': [0.6, 0.7],
            'min_price_chg_pct': [0.5, 1.0, 1.5],
            'allowed_mkt_struct': ['all', 'trend_only'],
            'min_trend_age_hours': [0.5, 1.0],
            'max_sr_test_count': [5, 999]
        }
    else:  # swing
        grid = {
            # ã€V8.4.4ã€‘åŸºç¡€å‚æ•°ï¼ˆåŠ¨æ€èŒƒå›´ï¼Œå›´ç»•åŸºå‡†Â±50%ï¼‰
            'max_holding_hours': generate_search_range('max_holding_hours', baseline['max_holding_hours']),
            'atr_tp_multiplier': generate_search_range('atr_tp_multiplier', baseline['atr_tp_multiplier']),
            'atr_stop_multiplier': generate_search_range('atr_stop_multiplier', baseline['atr_stop_multiplier']),
            'min_risk_reward': generate_search_range('min_risk_reward', baseline['min_risk_reward']),
            
            # å…¥åœºè¿‡æ»¤å‚æ•°ï¼ˆä¿æŒåŸæœ‰èŒƒå›´ï¼‰
            'min_signal_score': [40, 50, 60],
            'min_consensus_score': [0, 10, 20, 30],
            'min_consensus': [0, 1, 2],
            'min_kline_bullish_ratio': [0.6, 0.7],
            'min_price_chg_pct': [0.5, 1.0, 1.5],
            'allowed_mkt_struct': ['all', 'trend_only'],
            'min_trend_age_hours': [1.0, 2.0],
            'max_sr_test_count': [5, 999]
        }
    
    return grid


def random_sample_param_grid(grid: Dict, sample_size: int) -> List[Dict]:
    """
    ã€V8.4.5ã€‘æ™ºèƒ½é‡‡æ ·å‚æ•°ç»„åˆ
    
    ç­–ç•¥ï¼š
    1. è¾¹ç•Œé‡‡æ ·ï¼ˆ30%ï¼‰ï¼šæµ‹è¯•æ¯ä¸ªå‚æ•°çš„æå€¼ï¼Œç¡®ä¿è¦†ç›–è¾¹ç•Œ
    2. ä¸­å¿ƒç‚¹é‡‡æ ·ï¼ˆ1ä¸ªï¼‰ï¼šæµ‹è¯•é»˜è®¤é…ç½®
    3. éšæœºå¡«å……ï¼ˆå‰©ä½™ï¼‰ï¼šè¦†ç›–å…¶ä»–åŒºåŸŸ
    
    ä¼˜åŠ¿ï¼š
    - ç¡®ä¿æµ‹è¯•æ‰€æœ‰å…³é”®åŒºåŸŸï¼ˆè¾¹ç•Œã€ä¸­å¿ƒï¼‰
    - ä¸å¢åŠ è®¡ç®—é‡ï¼ˆä»ç„¶200ç»„ï¼‰
    - æé«˜æ‰¾åˆ°æœ€ä¼˜è§£çš„æ¦‚ç‡
    
    ç¤ºä¾‹ï¼ˆ200ç»„ï¼‰ï¼š
    - è¾¹ç•Œé‡‡æ ·ï¼š60ç»„ï¼ˆæ¯ä¸ªå‚æ•°çš„min/maxé…ç½®ï¼‰
    - ä¸­å¿ƒç‚¹ï¼š1ç»„
    - éšæœºï¼š139ç»„
    """
    samples = []
    param_names = list(grid.keys())
    param_values = [grid[name] for name in param_names]
    
    # ===== 1. è¾¹ç•Œé‡‡æ ·ï¼ˆ30%ï¼‰ =====
    boundary_samples = []
    for i, param_name in enumerate(param_names):
        values = param_values[i]
        if len(values) < 2:
            continue  # åªæœ‰1ä¸ªå€¼ï¼Œè·³è¿‡
        
        # æœ€å°å€¼é…ç½®ï¼šè¯¥å‚æ•°å–æœ€å°å€¼ï¼Œå…¶ä»–å‚æ•°å–ä¸­é—´å€¼
        min_config = {}
        for j, name in enumerate(param_names):
            if j == i:
                min_config[name] = param_values[j][0]  # æœ€å°å€¼
            else:
                mid_idx = len(param_values[j]) // 2
                min_config[name] = param_values[j][mid_idx]  # ä¸­é—´å€¼
        boundary_samples.append(min_config)
        
        # æœ€å¤§å€¼é…ç½®ï¼šè¯¥å‚æ•°å–æœ€å¤§å€¼ï¼Œå…¶ä»–å‚æ•°å–ä¸­é—´å€¼
        max_config = {}
        for j, name in enumerate(param_names):
            if j == i:
                max_config[name] = param_values[j][-1]  # æœ€å¤§å€¼
            else:
                mid_idx = len(param_values[j]) // 2
                max_config[name] = param_values[j][mid_idx]  # ä¸­é—´å€¼
        boundary_samples.append(max_config)
    
    # å»é‡ï¼ˆå¯èƒ½æœ‰é‡å¤çš„è¾¹ç•Œé…ç½®ï¼‰
    boundary_samples_unique = []
    seen = set()
    for config in boundary_samples:
        config_tuple = tuple(sorted(config.items()))
        if config_tuple not in seen:
            seen.add(config_tuple)
            boundary_samples_unique.append(config)
    
    samples.extend(boundary_samples_unique)
    
    # ===== 2. ä¸­å¿ƒç‚¹é‡‡æ ·ï¼ˆ1ä¸ªï¼‰ =====
    center_config = {}
    for i, name in enumerate(param_names):
        mid_idx = len(param_values[i]) // 2
        center_config[name] = param_values[i][mid_idx]
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    center_tuple = tuple(sorted(center_config.items()))
    if center_tuple not in seen:
        samples.append(center_config)
        seen.add(center_tuple)
    
    # ===== 3. éšæœºå¡«å……ï¼ˆå‰©ä½™ï¼‰ =====
    remaining = sample_size - len(samples)
    if remaining > 0:
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆçš„ç´¢å¼•
        from itertools import product
        all_indices = list(product(*[range(len(vals)) for vals in param_values]))
        
        # è¿‡æ»¤æ‰å·²é‡‡æ ·çš„é…ç½®
        available_indices = []
        for indices in all_indices:
            config = {
                param_names[i]: param_values[i][indices[i]]
                for i in range(len(param_names))
            }
            config_tuple = tuple(sorted(config.items()))
            if config_tuple not in seen:
                available_indices.append(indices)
        
        # éšæœºé‡‡æ ·
        if len(available_indices) > remaining:
            sampled_indices = random.sample(available_indices, remaining)
        else:
            sampled_indices = available_indices
        
        # æ„å»ºå‚æ•°å­—å…¸
        for indices in sampled_indices:
            config = {
                param_names[i]: param_values[i][indices[i]]
                for i in range(len(param_names))
            }
            samples.append(config)
    
    print(f"   ğŸ“Š æ™ºèƒ½é‡‡æ ·ç»Ÿè®¡:")
    print(f"      è¾¹ç•Œé‡‡æ ·: {len(boundary_samples_unique)}ç»„")
    print(f"      ä¸­å¿ƒç‚¹: 1ç»„")
    print(f"      éšæœºå¡«å……: {len(samples) - len(boundary_samples_unique) - 1}ç»„")
    print(f"      æ€»è®¡: {len(samples)}ç»„")
    
    return samples


def calculate_total_combinations(grid: Dict) -> int:
    """è®¡ç®—æ€»ç»„åˆæ•°"""
    total = 1
    for values in grid.values():
        total *= len(values)
    return total


# ============================================================
# ã€æ­¥éª¤3ã€‘V8.3.21ä¸Šä¸‹æ–‡è¿‡æ»¤å‡½æ•°
# ============================================================

def test_params_on_opportunities(opportunities: List[Dict], params: Dict) -> Dict:
    """
    ã€V8.4.5ã€‘æµ‹è¯•å‚æ•°åœ¨æœºä¼šé›†ä¸Šçš„è¡¨ç°ï¼ˆåˆ«åå‡½æ•°ï¼‰
    
    è¿™æ˜¯simulate_params_with_v8321_filterçš„åˆ«åï¼Œ
    ç”¨äºå‰å‘éªŒè¯æ—¶æµ‹è¯•å‚æ•°æ•ˆæœã€‚
    
    Args:
        opportunities: æœºä¼šåˆ—è¡¨
        params: å‚æ•°å­—å…¸
    
    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸ï¼ˆåŒ…æ‹¬avg_profit, capture_rateç­‰ï¼‰
    """
    return simulate_params_with_v8321_filter(opportunities, params)


def simulate_params_with_v8321_filter(opportunities: List[Dict], params: Dict) -> Dict:
    """
    ã€V8.3.21â†’V8.3.21.1ã€‘ä½¿ç”¨ä¸Šä¸‹æ–‡è¿‡æ»¤å‚æ•°æ¨¡æ‹Ÿäº¤æ˜“ï¼ˆä¿®å¤è¿‡åº¦è¿‡æ»¤ï¼‰
    
    è¿‡æ»¤å±‚æ¬¡ï¼š
    1. åŸºç¡€è¿‡æ»¤ï¼ˆsignal_score/consensus/risk_rewardï¼‰- å¿…é¡»
    2. Kçº¿ä¸Šä¸‹æ–‡è¿‡æ»¤ï¼ˆé˜³çº¿æ¯”ä¾‹ã€ä»·æ ¼å˜åŒ–ï¼‰- å¯é€‰
    3. å¸‚åœºç»“æ„è¿‡æ»¤ï¼ˆswingç±»å‹ã€è¶‹åŠ¿å¹´é¾„ï¼‰- å¯é€‰
    4. S/Rå†å²è¿‡æ»¤ï¼ˆæµ‹è¯•æ¬¡æ•°ã€å‡çªç ´ï¼‰- å¯é€‰
    
    ã€V8.3.21.1ä¿®å¤ã€‘ï¼šLayer 2-4é»˜è®¤ä¸å¯ç”¨ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤å†å²æ•°æ®
    """
    captured = []
    missed_reasons = {}
    
    # ã€V8.3.21.1ä¿®å¤ã€‘é«˜çº§è¿‡æ»¤å™¨é»˜è®¤ä¸å¯ç”¨
    enable_advanced_filters = params.get('enable_advanced_filters', False)
    
    for opp in opportunities:
        # ç¬¬1å±‚ï¼šåŸºç¡€è¿‡æ»¤ï¼ˆå¿…é¡»ï¼‰
        if not passes_basic_filter(opp, params):
            reason = 'basic_params'
            missed_reasons[reason] = missed_reasons.get(reason, 0) + 1
            continue
        
        # ã€V8.3.21.1ä¿®å¤ã€‘ç¬¬2-4å±‚ï¼šé«˜çº§è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸å¯ç”¨ï¼‰
        if enable_advanced_filters:
            # ç¬¬2å±‚ï¼šKçº¿ä¸Šä¸‹æ–‡è¿‡æ»¤
            if not passes_kline_context_filter(opp, params):
                reason = 'kline_context'
                missed_reasons[reason] = missed_reasons.get(reason, 0) + 1
                continue
            
            # ç¬¬3å±‚ï¼šå¸‚åœºç»“æ„è¿‡æ»¤
            if not passes_market_structure_filter(opp, params):
                reason = 'market_structure'
                missed_reasons[reason] = missed_reasons.get(reason, 0) + 1
                continue
            
            # ç¬¬4å±‚ï¼šS/Rå†å²è¿‡æ»¤
            if not passes_sr_history_filter(opp, params):
                reason = 'sr_history'
                missed_reasons[reason] = missed_reasons.get(reason, 0) + 1
                continue
        
        # é€šè¿‡æ‰€æœ‰è¿‡æ»¤ï¼Œè®°å½•
        captured.append(opp)
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if len(captured) == 0:
        return {
            'total_opportunities': len(opportunities),
            'captured_count': 0,
            'capture_rate': 0,
            'avg_profit': 0,
            'win_rate': 0,
            'time_exit_rate': 0,
            'missed_reasons': missed_reasons
        }
    
    # ã€V8.3.21.1ä¿®å¤ã€‘è®¡ç®—åˆ©æ¶¦ï¼ˆå…¼å®¹ä¸åŒå­—æ®µåï¼‰
    # ä¼˜å…ˆä½¿ç”¨actual_profit_pctï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨objective_profit
    profits = []
    for c in captured:
        if 'actual_profit_pct' in c:
            profits.append(c['actual_profit_pct'])
        elif 'objective_profit' in c:
            profits.append(c['objective_profit'])
        else:
            profits.append(0)  # é»˜è®¤å€¼
    
    avg_profit = np.mean(profits) if len(profits) > 0 else 0
    
    # ã€V8.3.21é£æ§ã€‘åˆ†ç¦»ç›ˆåˆ©å’ŒäºæŸ
    wins = [p for p in profits if p > 0]
    losses = [p for p in profits if p <= 0]
    
    win_rate = len(wins) / len(profits) if len(profits) > 0 else 0
    avg_win = np.mean(wins) if len(wins) > 0 else 0
    avg_loss = np.mean(losses) if len(losses) > 0 else 0
    
    # ç›ˆäºæ¯”ï¼ˆèµšçš„æ—¶å€™èµšå¤šå°‘ / äºçš„æ—¶å€™äºå¤šå°‘ï¼‰
    profit_loss_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else 999
    
    # æœŸæœ›æ”¶ç›Šï¼ˆè€ƒè™‘èƒœç‡å’Œç›ˆäºæ¯”ï¼‰
    expectancy = (win_rate * avg_win) + ((1 - win_rate) * avg_loss)
    
    # æœ€å¤§å›æ’¤ï¼ˆè¿ç»­äºæŸçš„æœ€å¤§å€¼ï¼‰
    max_drawdown = 0
    cumulative = 0
    peak = 0
    for p in profits:
        cumulative += p
        if cumulative > peak:
            peak = cumulative
        drawdown = peak - cumulative
        if drawdown > max_drawdown:
            max_drawdown = drawdown
    
    return {
        'total_opportunities': len(opportunities),
        'captured_count': len(captured),
        'capture_rate': len(captured) / len(opportunities),
        'avg_profit': avg_profit,
        'win_rate': win_rate,
        'time_exit_rate': 0.5,  # ç®€åŒ–ï¼šå‡è®¾50% time_exit
        'missed_reasons': missed_reasons,
        'captured_details': captured,  # è¯¦ç»†æ•°æ®ï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰
        
        # ã€V8.3.21é£æ§ã€‘æ–°å¢é£æ§æŒ‡æ ‡
        'avg_win': avg_win,                    # ç›ˆåˆ©æ—¶å¹³å‡èµšå¤šå°‘
        'avg_loss': avg_loss,                  # äºæŸæ—¶å¹³å‡äºå¤šå°‘
        'profit_loss_ratio': profit_loss_ratio, # ç›ˆäºæ¯”
        'expectancy': expectancy,              # æœŸæœ›æ”¶ç›Š
        'max_drawdown': max_drawdown,          # æœ€å¤§å›æ’¤
        'win_count': len(wins),                # ç›ˆåˆ©ç¬”æ•°
        'loss_count': len(losses)              # äºæŸç¬”æ•°
    }


def get_profit_pct(opp: Dict) -> float:
    """
    ã€V8.3.21.1è¾…åŠ©ã€‘è·å–åˆ©æ¶¦ç™¾åˆ†æ¯”ï¼ˆå…¼å®¹ä¸åŒå­—æ®µåï¼‰
    
    ä¼˜å…ˆçº§ï¼šactual_profit_pct > objective_profit > 0
    """
    if 'actual_profit_pct' in opp:
        return opp['actual_profit_pct']
    elif 'objective_profit' in opp:
        return opp['objective_profit']
    else:
        return 0.0


def passes_basic_filter(opp: Dict, params: Dict) -> bool:
    """
    ã€V8.4ã€‘åŸºç¡€å‚æ•°è¿‡æ»¤ - ä½¿ç”¨æ–°çš„consensus_score
    
    ä¼˜å…ˆçº§ï¼š
    1. ä¼˜å…ˆä½¿ç”¨consensus_scoreï¼ˆ0-100ï¼‰è€Œéæ—§çš„consensusï¼ˆ0-5ï¼‰
    2. ä¼˜å…ˆä½¿ç”¨actual_risk_rewardè€Œérisk_reward
    """
    # ä¼˜å…ˆä½¿ç”¨actual_risk_rewardï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°risk_reward
    rr_value = opp.get('actual_risk_reward', opp.get('risk_reward', 0))
    
    # ã€V8.4ã€‘ä¼˜å…ˆä½¿ç”¨æ–°çš„consensus_scoreï¼ˆ0-100åˆ†ï¼‰
    if 'consensus_score' in opp and 'min_consensus_score' in params:
        # æ–°ç‰ˆè¿‡æ»¤ï¼šä½¿ç”¨consensus_score
        return (
            opp['signal_score'] >= params.get('min_signal_score', 50) and
            opp['consensus_score'] >= params.get('min_consensus_score', 30) and
            rr_value >= params.get('min_risk_reward', 1.5)
        )
    else:
        # ã€å…¼å®¹æ€§ã€‘å›é€€åˆ°æ—§ç‰ˆè¿‡æ»¤ï¼šä½¿ç”¨consensusï¼ˆ0-5ï¼‰
        consensus_value = opp.get('consensus', opp.get('indicator_consensus', 0))
        return (
            opp['signal_score'] >= params.get('min_signal_score', 50) and
            consensus_value >= params.get('min_consensus', 2) and
            rr_value >= params.get('min_risk_reward', 1.5)
        )



def passes_kline_context_filter(opp: Dict, params: Dict) -> bool:
    """Kçº¿ä¸Šä¸‹æ–‡è¿‡æ»¤"""
    # æ£€æŸ¥é˜³çº¿/é˜´çº¿æ¯”ä¾‹
    bullish_ratio = opp.get('kline_ctx_bullish_ratio', 0)
    min_ratio = params.get('min_kline_bullish_ratio', 0.6)
    
    if opp['direction'] == 'long':
        if bullish_ratio < min_ratio:
            return False
    else:  # short
        if (1 - bullish_ratio) < min_ratio:
            return False
    
    # æ£€æŸ¥ä»·æ ¼å˜åŒ–å¹…åº¦
    price_chg = abs(opp.get('kline_ctx_price_chg_pct', 0))
    if price_chg < params.get('min_price_chg_pct', 0.5):
        return False
    
    return True


def passes_market_structure_filter(opp: Dict, params: Dict) -> bool:
    """å¸‚åœºç»“æ„è¿‡æ»¤"""
    # æ£€æŸ¥æ˜¯å¦åªåšè¶‹åŠ¿å¸‚åœº
    if params.get('allowed_mkt_struct') == 'trend_only':
        swing_type = opp.get('mkt_struct_swing', '')
        if swing_type not in ['HH-HL', 'LL-LH']:
            return False
    
    # æ£€æŸ¥è¶‹åŠ¿å¹´é¾„
    trend_age = opp.get('mkt_struct_age_hours', 0)
    min_age = params.get('min_trend_age_hours', 0.5)
    if trend_age < min_age:
        return False
    
    return True


def passes_sr_history_filter(opp: Dict, params: Dict) -> bool:
    """S/Rå†å²è¿‡æ»¤"""
    # æ ¹æ®æ–¹å‘é€‰æ‹©å¯¹åº”çš„S/R
    if opp['direction'] == 'long':
        test_cnt = opp.get('support_hist_test_cnt', 0)
        false_bd = opp.get('support_hist_false_bd', 0)
    else:
        test_cnt = opp.get('resist_hist_test_cnt', 0)
        false_bd = opp.get('resist_hist_false_bo', 0)
    
    # æ£€æŸ¥æµ‹è¯•æ¬¡æ•°
    max_test = params.get('max_sr_test_count', 999)
    if test_cnt > max_test:
        return False
    
    # æ£€æŸ¥å‡çªç ´ï¼ˆå›ºå®šâ‰¤2æ¬¡ï¼‰
    if false_bd > 2:
        return False
    
    return True


# ============================================================
# ã€æ­¥éª¤4ã€‘æœ¬åœ°ç»Ÿè®¡åˆ†æå‡½æ•°
# ============================================================

def calculate_param_sensitivity_local(all_results: List[Dict]) -> Dict:
    """
    ã€æœ¬åœ°è®¡ç®—ã€‘å‚æ•°æ•æ„Ÿåº¦åˆ†æ
    
    è®¡ç®—æ¯ä¸ªå‚æ•°å˜åŒ–æ—¶ï¼Œscoreçš„å¹³å‡å˜åŒ–é‡
    """
    sensitivity = {}
    
    # æŒ‰å‚æ•°åˆ†ç»„
    param_names = list(all_results[0]['params'].keys())
    
    for param_name in param_names:
        # è·å–è¯¥å‚æ•°çš„æ‰€æœ‰å–å€¼
        param_values = sorted(set([r['params'][param_name] for r in all_results]))
        
        if len(param_values) < 2:
            continue
        
        # è®¡ç®—ç›¸é‚»å–å€¼ä¹‹é—´çš„scoreå˜åŒ–
        score_changes = []
        for i in range(len(param_values) - 1):
            v1, v2 = param_values[i], param_values[i+1]
            
            # æ‰¾åˆ°è¯¥å‚æ•°=v1å’Œv2çš„ç»“æœ
            results_v1 = [r for r in all_results if r['params'][param_name] == v1]
            results_v2 = [r for r in all_results if r['params'][param_name] == v2]
            
            if results_v1 and results_v2:
                avg_score_v1 = np.mean([r['score'] for r in results_v1])
                avg_score_v2 = np.mean([r['score'] for r in results_v2])
                
                # è®¡ç®—å•ä½å˜åŒ–çš„å½±å“
                param_change = abs(v2 - v1) if isinstance(v1, (int, float)) else 1
                score_change = (avg_score_v2 - avg_score_v1) / param_change
                score_changes.append(score_change)
        
        if score_changes:
            avg_impact = np.mean(score_changes)
            sensitivity[param_name] = {
                'avg_impact': round(avg_impact, 3),
                'std_impact': round(np.std(score_changes), 3),
                'importance': 'high' if abs(avg_impact) > 0.1 else 'medium' if abs(avg_impact) > 0.05 else 'low'
            }
    
    return sensitivity


def analyze_context_features_local(opportunities: List[Dict], best_params: Dict) -> Dict:
    """
    ã€æœ¬åœ°è®¡ç®—ã€‘ä¸Šä¸‹æ–‡ç‰¹å¾åˆ†æ
    
    åˆ†æV8.3.21å­—æ®µä¸æˆåŠŸçš„å…³ç³»
    """
    # ä½¿ç”¨æœ€ä¼˜å‚æ•°æ¨¡æ‹Ÿï¼ŒåŒºåˆ†capturedå’Œmissed
    result = simulate_params_with_v8321_filter(opportunities, best_params)
    captured = result.get('captured_details', [])
    
    if len(captured) == 0:
        return {'error': 'æ— æ•è·æœºä¼š'}
    
    analysis = {}
    
    # åˆ†æ1ï¼šKçº¿ä¸Šä¸‹æ–‡
    analysis['kline_context'] = analyze_kline_context_impact(captured)
    
    # åˆ†æ2ï¼šå¸‚åœºç»“æ„
    analysis['market_structure'] = analyze_market_structure_impact(captured)
    
    # åˆ†æ3ï¼šS/Rå†å²
    analysis['sr_history'] = analyze_sr_history_impact(captured)
    
    # ç”Ÿæˆå…³é”®æ´å¯Ÿ
    analysis['key_insights'] = generate_insights_from_analysis(analysis)
    
    return analysis


def analyze_kline_context_impact(captured: List[Dict]) -> Dict:
    """åˆ†æKçº¿ä¸Šä¸‹æ–‡ä¸æˆåŠŸç‡çš„å…³ç³»"""
    # æŒ‰é˜³çº¿æ¯”ä¾‹åˆ†ç»„
    groups = {
        '0.6-0.7': [],
        '0.7-0.8': [],
        '0.8-1.0': []
    }
    
    for opp in captured:
        ratio = opp.get('kline_ctx_bullish_ratio', 0)
        if 0.6 <= ratio < 0.7:
            groups['0.6-0.7'].append(opp)
        elif 0.7 <= ratio < 0.8:
            groups['0.7-0.8'].append(opp)
        elif 0.8 <= ratio <= 1.0:
            groups['0.8-1.0'].append(opp)
    
    # è®¡ç®—å„ç»„ç»Ÿè®¡
    result = {}
    for range_name, group in groups.items():
        if len(group) > 0:
            profits = [get_profit_pct(o) for o in group]  # ã€V8.3.21.1ä¿®å¤ã€‘ä½¿ç”¨è¾…åŠ©å‡½æ•°
            result[range_name] = {
                'count': len(group),
                'avg_profit': round(np.mean(profits), 1),
                'win_rate': round(len([p for p in profits if p > 0]) / len(profits), 2)
            }
    
    # ç”Ÿæˆç»“è®º
    if result:
        best_range = max(result.keys(), key=lambda k: result[k]['avg_profit'])
        result['conclusion'] = f"é˜³çº¿æ¯”ä¾‹{best_range}æ—¶æ•ˆæœæœ€å¥½ï¼ˆå¹³å‡åˆ©æ¶¦{result[best_range]['avg_profit']:.1f}%ï¼‰"
    
    return result


def analyze_market_structure_impact(captured: List[Dict]) -> Dict:
    """åˆ†æå¸‚åœºç»“æ„ä¸æˆåŠŸç‡çš„å…³ç³»"""
    # æŒ‰swingç±»å‹åˆ†ç»„
    groups = {}
    for opp in captured:
        swing_type = opp.get('mkt_struct_swing', 'unknown')
        if swing_type not in groups:
            groups[swing_type] = []
        groups[swing_type].append(opp)
    
    # è®¡ç®—å„ç»„ç»Ÿè®¡
    result = {}
    for swing_type, group in groups.items():
        if len(group) > 0:
            profits = [get_profit_pct(o) for o in group]  # ã€V8.3.21.1ä¿®å¤ã€‘ä½¿ç”¨è¾…åŠ©å‡½æ•°
            result[swing_type] = {
                'count': len(group),
                'avg_profit': round(np.mean(profits), 1)
            }
    
    # ç”Ÿæˆç»“è®º
    if result:
        best_type = max(result.keys(), key=lambda k: result[k]['avg_profit'])
        result['conclusion'] = f"{best_type}ç»“æ„æ•ˆæœæœ€å¥½ï¼ˆå¹³å‡åˆ©æ¶¦{result[best_type]['avg_profit']:.1f}%ï¼‰"
    
    return result


def analyze_sr_history_impact(captured: List[Dict]) -> Dict:
    """åˆ†æS/Rå†å²ä¸æˆåŠŸç‡çš„å…³ç³»"""
    # æŒ‰æµ‹è¯•æ¬¡æ•°åˆ†ç»„
    groups = {
        '1-2æ¬¡': [],
        '3-5æ¬¡': [],
        '5æ¬¡+': []
    }
    
    for opp in captured:
        test_cnt = opp.get('resist_hist_test_cnt', 0) if opp['direction'] == 'short' else opp.get('support_hist_test_cnt', 0)
        
        if 1 <= test_cnt <= 2:
            groups['1-2æ¬¡'].append(opp)
        elif 3 <= test_cnt <= 5:
            groups['3-5æ¬¡'].append(opp)
        elif test_cnt > 5:
            groups['5æ¬¡+'].append(opp)
    
    # è®¡ç®—å„ç»„ç»Ÿè®¡
    result = {}
    for range_name, group in groups.items():
        if len(group) > 0:
            profits = [get_profit_pct(o) for o in group]  # ã€V8.3.21.1ä¿®å¤ã€‘ä½¿ç”¨è¾…åŠ©å‡½æ•°
            result[range_name] = {
                'count': len(group),
                'avg_profit': round(np.mean(profits), 1)
            }
    
    # ç”Ÿæˆç»“è®º
    if result:
        best_range = max(result.keys(), key=lambda k: result[k]['avg_profit'])
        result['conclusion'] = f"S/Ræµ‹è¯•{best_range}æ—¶æ•ˆæœæœ€å¥½ï¼ˆå¹³å‡åˆ©æ¶¦{result[best_range]['avg_profit']:.1f}%ï¼‰"
    
    return result


def generate_insights_from_analysis(analysis: Dict) -> List[str]:
    """ä»åˆ†æä¸­ç”Ÿæˆå…³é”®æ´å¯Ÿ"""
    insights = []
    
    # Kçº¿ä¸Šä¸‹æ–‡æ´å¯Ÿ
    if 'kline_context' in analysis and 'conclusion' in analysis['kline_context']:
        insights.append(f"ğŸ’¡ {analysis['kline_context']['conclusion']}")
    
    # å¸‚åœºç»“æ„æ´å¯Ÿ
    if 'market_structure' in analysis and 'conclusion' in analysis['market_structure']:
        insights.append(f"ğŸ’¡ {analysis['market_structure']['conclusion']}")
    
    # S/Rå†å²æ´å¯Ÿ
    if 'sr_history' in analysis and 'conclusion' in analysis['sr_history']:
        insights.append(f"ğŸ’¡ {analysis['sr_history']['conclusion']}")
    
    return insights


def detect_anomalies_local(all_results: List[Dict], param_sensitivity: Dict) -> List[Dict]:
    """
    ã€æœ¬åœ°æ£€æµ‹ã€‘å¼‚å¸¸æƒ…å†µ
    
    åŸºäºè§„åˆ™æ£€æµ‹å¼‚å¸¸ï¼Œä¸éœ€è¦AI
    """
    anomalies = []
    
    # å¼‚å¸¸1ï¼šæŸä¸ªå‚æ•°å¯¼è‡´æ•è·ç‡éª¤é™
    for param_name in param_sensitivity.keys():
        # æ‰¾åˆ°è¯¥å‚æ•°çš„æç«¯å€¼
        param_results = {}
        for r in all_results:
            pval = r['params'][param_name]
            if pval not in param_results:
                param_results[pval] = []
            param_results[pval].append(r['metrics'].get('capture_rate', 0))
        
        # è®¡ç®—æ¯ä¸ªå€¼çš„å¹³å‡æ•è·ç‡
        param_avg_capture = {k: np.mean(v) for k, v in param_results.items()}
        
        # æ£€æµ‹éª¤é™
        values = sorted(param_avg_capture.keys())
        for i in range(len(values) - 1):
            v1, v2 = values[i], values[i+1]
            drop = param_avg_capture[v2] - param_avg_capture[v1]
            
            if drop < -0.2:  # ä¸‹é™è¶…è¿‡20%
                anomalies.append({
                    'type': 'capture_rate_drop',
                    'param': param_name,
                    'from_value': v1,
                    'to_value': v2,
                    'drop': round(drop, 2),
                    'severity': 'high' if drop < -0.3 else 'medium',
                    'description': f'{param_name}ä»{v1}â†’{v2}æ—¶ï¼Œæ•è·ç‡ä¸‹é™{abs(drop)*100:.0f}%'
                })
    
    # å¼‚å¸¸2ï¼šæ•´ä½“æ•è·ç‡è¿‡ä½
    avg_capture_rate = np.mean([r['metrics'].get('capture_rate', 0) for r in all_results])
    if avg_capture_rate < 0.3:
        anomalies.append({
            'type': 'low_capture_rate',
            'value': round(avg_capture_rate, 2),
            'severity': 'high',
            'description': f'æ•´ä½“æ•è·ç‡è¿‡ä½ï¼ˆ{avg_capture_rate*100:.0f}%ï¼‰ï¼Œå‚æ•°å¯èƒ½è¿‡ä¸¥'
        })
    
    return anomalies


# ============================================================
# ã€æ­¥éª¤5ã€‘è¯„åˆ†å‡½æ•°å’Œè¾…åŠ©å‡½æ•°
# ============================================================

def calculate_v8321_optimization_score(result: Dict) -> float:
    """
    ã€V8.3.21åˆ©æ¶¦æœ€å¤§åŒ–+é£æ§ã€‘è¯„åˆ†å‡½æ•°
    
    æ ¸å¿ƒç›®æ ‡ï¼šåˆ©æ¶¦æœ€å¤§ + äºæŸæœ€å°
    
    å…³é”®æŒ‡æ ‡ï¼š
    1. æœŸæœ›æ”¶ç›Š = (èƒœç‡ Ã— å¹³å‡ç›ˆåˆ©) + (è´¥ç‡ Ã— å¹³å‡äºæŸ)
       - ç»¼åˆè€ƒè™‘ç›ˆåˆ©å’ŒäºæŸ
    2. ç›ˆäºæ¯” = å¹³å‡ç›ˆåˆ© / |å¹³å‡äºæŸ|
       - èµšçš„æ—¶å€™èµšå¤šå°‘ vs äºçš„æ—¶å€™äºå¤šå°‘
    3. æœ€å¤§å›æ’¤
       - è¿ç»­äºæŸçš„æœ€å¤§å¹…åº¦ï¼ˆé£é™©æŒ‡æ ‡ï¼‰
    4. æ•è·ç‡
       - ç¡®ä¿ä¸è¿‡åº¦ä¿å®ˆ
    
    æƒé‡è®¾è®¡ï¼ˆå¯¹é½"åˆ©æ¶¦æœ€å¤§+äºæŸæœ€å°"ï¼‰ï¼š
    - æœŸæœ›æ”¶ç›Š: 50%ï¼ˆæ ¸å¿ƒï¼Œå·²åŒ…å«ç›ˆäºï¼‰ã€ä»40%æå‡åˆ°50%ã€‘
    - ç›ˆäºæ¯”: 25%ï¼ˆèµšå¤šäºå°‘ï¼‰ã€ä»30%é™åˆ°25%ã€‘
    - æ•è·ç‡: 15%ï¼ˆä¸è¿‡åº¦ä¿å®ˆï¼‰ã€ä»20%é™åˆ°15%ã€‘
    - å›æ’¤æƒ©ç½š: -10%ï¼ˆæ§åˆ¶é£é™©ï¼‰
    
    ç¤ºä¾‹ï¼š
    - é…ç½®A: 3%æœŸæœ› + 2.0ç›ˆäºæ¯” + 50%æ•è· - 5%å›æ’¤ = é«˜åˆ†
    - é…ç½®B: 3%æœŸæœ› + 1.2ç›ˆäºæ¯” + 50%æ•è· - 8%å›æ’¤ = ä½åˆ†ï¼ˆäºæŸå¤§ï¼‰
    """
    if result['captured_count'] == 0:
        return 0.0
    
    # ã€V8.4.5ã€‘æ”¹è¿›è¯„åˆ†é€»è¾‘
    avg_profit = result.get('avg_profit', 0)
    expectancy = result.get('expectancy', 0)
    win_rate = result.get('win_rate', 0)
    capture_rate = result.get('capture_rate', 0)
    
    # ã€V8.4.5æ–°å¢ã€‘è®¡ç®—time_exitæ¯”ä¾‹
    time_exit_count = result.get('time_exit_count', 0)
    total_count = result.get('captured_count', 1)
    time_exit_ratio = time_exit_count / total_count if total_count > 0 else 0
    
    # è´Ÿåˆ©æ¶¦ä»ç„¶ç»™ä½åˆ†ï¼ˆä¿ç•™V8.3.21.15çš„é€»è¾‘ï¼‰
    if avg_profit <= 0 or expectancy <= 0:
        base_penalty = 0.01
        if avg_profit > -2:
            return base_penalty + (avg_profit + 2) / 2 * 0.04
        else:
            return base_penalty
    
    # å½’ä¸€åŒ–å„æŒ‡æ ‡
    avg_profit_score = min(1.0, max(0, avg_profit / 20))  # 0%~20% â†’ 0~1
    expectancy_score = min(1.0, max(0, (expectancy + 5) / 15))  # -5%~+10% â†’ 0~1
    win_rate_score = win_rate  # 0~1
    capture_rate_score = capture_rate  # 0~1
    
    # ã€V8.4.5æ–°å¢ã€‘TP/SLè§¦å‘ç‡ï¼ˆtime_exitçš„åé¢ï¼‰
    tp_sl_trigger_rate = 1 - time_exit_ratio
    tp_sl_trigger_score = tp_sl_trigger_rate  # 0~1
    
    # ã€V8.4.5æ–°å¢ã€‘time_exitæƒ©ç½šï¼šå¦‚æœæ¯”ä¾‹>50%ï¼Œç»™äºˆæƒ©ç½š
    time_exit_penalty = max(0, (time_exit_ratio - 0.5) * 0.5)  # è¶…è¿‡50%ï¼Œæ¯10%æ‰£0.05åˆ†
    
    # ã€V8.4.5ã€‘è°ƒæ•´æƒé‡ï¼Œæé«˜æ•è·ç‡é‡è¦æ€§
    total_score = (
        avg_profit_score * 0.30 +       # å¹³å‡åˆ©æ¶¦ 30%ï¼ˆä»40%é™ä½ï¼‰
        expectancy_score * 0.20 +       # æœŸæœ›æ”¶ç›Š 20%ï¼ˆä»25%é™ä½ï¼‰
        win_rate_score * 0.15 +         # èƒœç‡ 15%ï¼ˆä¿æŒï¼‰
        capture_rate_score * 0.25 +     # æ•è·ç‡ 25%ï¼ˆä»0%æå‡ï¼‰â­
        tp_sl_trigger_score * 0.10 +    # TP/SLè§¦å‘ç‡ 10%ï¼ˆæ–°å¢ï¼‰
        - time_exit_penalty             # time_exitæƒ©ç½šï¼ˆåŠ¨æ€ï¼‰
    )
    
    return max(0, total_score)  # ç¡®ä¿éè´Ÿ


def extract_key_metrics(result: Dict) -> Dict:
    """æå–å…³é”®æŒ‡æ ‡"""
    return {
        'capture_rate': result.get('capture_rate', 0),
        'avg_profit': result.get('avg_profit', 0),
        'win_rate': result.get('win_rate', 0),
        'time_exit_rate': result.get('time_exit_rate', 0)
    }


def calculate_score_distribution(all_results: List[Dict]) -> Dict:
    """è®¡ç®—åˆ†æ•°åˆ†å¸ƒ"""
    scores = [r['score'] for r in all_results]
    return {
        'mean': round(np.mean(scores), 3),
        'std': round(np.std(scores), 3),
        'min': round(np.min(scores), 3),
        'max': round(np.max(scores), 3),
        'q25': round(np.percentile(scores, 25), 3),
        'q50': round(np.percentile(scores, 50), 3),
        'q75': round(np.percentile(scores, 75), 3)
    }


def compress_optimization_results(top_10: List[Dict], 
                                   param_sensitivity: Dict,
                                   context_analysis: Dict,
                                   anomalies: List[Dict]) -> Dict:
    """
    å‹ç¼©ä¼˜åŒ–ç»“æœï¼ˆç”¨äºAIå†³ç­–ï¼‰
    
    å°†è¯¦ç»†æ•°æ®å‹ç¼©æˆæ‘˜è¦
    """
    return {
        'top_3_configs': [
            {
                'rank': i + 1,
                'score': r['score'],
                'params_summary': format_params_compact(r['params']),
                'metrics': r['metrics']
            }
            for i, r in enumerate(top_10[:3])
        ],
        'param_sensitivity_summary': {
            k: v for k, v in sorted(param_sensitivity.items(), 
                                    key=lambda x: abs(x[1]['avg_impact']), 
                                    reverse=True)[:5]  # åªä¿ç•™Top 5
        },
        'context_insights': context_analysis.get('key_insights', []),
        'anomalies_summary': [
            {
                'type': a['type'],
                'severity': a['severity'],
                'description': a['description']
            }
            for a in anomalies[:3]  # åªä¿ç•™Top 3
        ]
    }


def format_params_compact(params: Dict) -> str:
    """ç´§å‡‘æ ¼å¼åŒ–å‚æ•°"""
    return ', '.join([f"{k}={v}" for k, v in list(params.items())[:3]]) + '...'


def estimate_token_count(data: Dict) -> int:
    """ä¼°ç®—tokenæ•°é‡"""
    import json
    json_str = json.dumps(data)
    # ç²—ç•¥ä¼°ç®—ï¼šæ¯4ä¸ªå­—ç¬¦â‰ˆ1 token
    return len(json_str) // 4


# ============================================================
# ä¸»å‡½æ•°ç¤ºä¾‹
# ============================================================

# ============================================================
# ã€AIè¿­ä»£å†³ç­–å±‚ã€‘è‹±æ–‡é€šä¿¡ï¼Œç”¨äºå¤šè½®ä¼˜åŒ–
# ============================================================

def call_ai_for_iterative_optimization(top_10_configs: List[Dict],
                                        param_sensitivity: Dict,
                                        context_analysis: Dict,
                                        anomalies: List[Dict],
                                        compressed_data: Dict,
                                        signal_type: str) -> Dict:
    """
    ã€V8.3.21 AIè¿­ä»£ã€‘Call AI for iterative parameter optimization
    
    Communication: English (efficient for AI)
    Output: English (internal use only, translated to Chinese for users)
    
    Args:
        top_10_configs: Top 10 parameter configurations from Grid Search
        param_sensitivity: Parameter sensitivity analysis
        context_analysis: Market context analysis
        anomalies: Detected anomalies
        compressed_data: Compressed optimization data
        signal_type: 'scalping' or 'swing'
    
    Returns:
        AI decision dict (English)
    """
    
    # æ„å»ºè‹±æ–‡Prompt
    prompt = build_ai_optimization_prompt_en(
        top_10_configs=top_10_configs,
        param_sensitivity=param_sensitivity,
        context_analysis=context_analysis,
        anomalies=anomalies,
        signal_type=signal_type
    )
    
    # è°ƒç”¨AIï¼ˆè‹±æ–‡é€šä¿¡ï¼‰
    ai_response = call_deepseek_for_optimization(prompt)
    
    # è§£æAIå“åº”ï¼ˆè‹±æ–‡ï¼‰
    ai_decision = parse_ai_optimization_response(ai_response)
    
    # è½¬æ¢å…³é”®æ´å¯Ÿä¸ºä¸­æ–‡ï¼ˆç»™ç”¨æˆ·çœ‹ï¼‰
    if ai_decision:
        ai_decision['key_insights_zh'] = translate_insights_to_chinese(
            ai_decision.get('key_insights_en', [])
        )
    
    return ai_decision


def build_ai_optimization_prompt_en(top_10_configs: List[Dict],
                                      param_sensitivity: Dict,
                                      context_analysis: Dict,
                                      anomalies: List[Dict],
                                      signal_type: str) -> str:
    """
    Build English prompt for AI optimization
    
    English communication is more efficient for AI reasoning
    """
    
    # Format Top 3 configs
    top_3_str = ""
    for i, config in enumerate(top_10_configs[:3], 1):
        top_3_str += f"\nRank {i}:\n"
        top_3_str += f"  Score: {config['score']:.3f}\n"
        top_3_str += f"  Capture Rate: {config['metrics']['capture_rate']*100:.0f}%\n"
        top_3_str += f"  Avg Profit: {config['metrics']['avg_profit']:.1f}%\n"
        top_3_str += f"  Win Rate: {config['metrics']['win_rate']*100:.0f}%\n"
        # Show key params
        params = config['params']
        top_3_str += f"  Key Params: signalâ‰¥{params.get('min_signal_score', 60)}, "
        top_3_str += f"consensusâ‰¥{params.get('min_consensus', 3)}, "
        top_3_str += f"RRâ‰¥{params.get('min_risk_reward', 2.0):.1f}\n"
    
    # Format parameter sensitivity (Top 3)
    sensitivity_str = ""
    sorted_params = sorted(
        param_sensitivity.items(),
        key=lambda x: abs(x[1]['avg_impact']),
        reverse=True
    )[:3]
    for param_name, sensitivity in sorted_params:
        sensitivity_str += f"\n  â€¢ {param_name}: {sensitivity['importance']} importance"
        sensitivity_str += f" (impact={sensitivity['avg_impact']:+.3f})"
    
    # Format context insights
    insights_str = "\n".join([f"  â€¢ {insight}" for insight in context_analysis.get('key_insights', [])[:3]])
    
    # Format anomalies
    anomalies_str = ""
    for anomaly in anomalies[:2]:
        anomalies_str += f"\n  â€¢ {anomaly.get('type', 'unknown')}: {anomaly.get('description', 'N/A')}"
    
    # Construct prompt (English)
    prompt = f"""You are an expert in trading parameter optimization. Analyze the Grid Search results and provide iterative improvement suggestions.

Signal Type: {signal_type.upper()}

=== Grid Search Results (200 combinations tested) ===

Top 3 Configurations:
{top_3_str}

=== Parameter Sensitivity Analysis ===
{sensitivity_str}

=== Market Context Insights ===
{insights_str}

=== Detected Anomalies ===
{anomalies_str if anomalies_str else "  None"}

=== Your Task ===

Based on the above analysis:

1. Should we accept Rank 1, or try a different configuration?
2. Can we make micro-adjustments to improve the score further?
3. What are the key risks in the current market context?
4. Any recommendations for next iteration?

Please respond in JSON format:

{{
    "needs_adjustment": true/false,
    "selected_rank": 1,  // 1-3
    "param_adjustments": {{
        // Only specify params that need adjustment
        // Example: "min_signal_score": 65
    }},
    "reasoning_en": "Brief explanation why this choice is optimal",
    "key_insights_en": [
        "Insight 1",
        "Insight 2"
    ],
    "risk_warning_en": "Market risk assessment"
}}

Respond with ONLY the JSON, no additional text."""

    return prompt


def call_deepseek_for_optimization(prompt: str) -> str:
    """
    Call DeepSeek API for optimization decision
    
    Uses existing call_deepseek function from main file
    """
    try:
        # å°è¯•å¯¼å…¥ä¸»æ–‡ä»¶çš„call_deepseekå‡½æ•°
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
        
        # åŠ¨æ€å¯¼å…¥ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
        import importlib.util
        spec = importlib.util.spec_from_file_location("main", "qwen_å¤šå¸ç§æ™ºèƒ½ç‰ˆ.py")
        if spec and spec.loader:
            main_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(main_module)
            
            if hasattr(main_module, 'call_deepseek'):
                response = main_module.call_deepseek(
                    prompt=prompt,
                    max_tokens=500,
                    temperature=0.3
                )
                return response
        
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¿”å›Noneï¼ˆè·³è¿‡AIå†³ç­–ï¼‰
        return None
        
    except Exception as e:
        print(f"âš ï¸  AI APIè°ƒç”¨å¤±è´¥: {e}")
        return None


def parse_ai_optimization_response(ai_response: str) -> Dict:
    """
    Parse AI response (JSON format)
    
    Returns English decision dict
    """
    if not ai_response:
        return None
    
    try:
        import json
        import re
        
        # æå–JSONï¼ˆAIå¯èƒ½ä¼šè¿”å›é¢å¤–çš„æ–‡å­—ï¼‰
        json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            decision = json.loads(json_str)
            return decision
        else:
            return None
            
    except Exception as e:
        print(f"âš ï¸  AIå“åº”è§£æå¤±è´¥: {e}")
        return None


def apply_ai_adjustments(base_params: Dict, adjustments: Dict) -> Dict:
    """
    Apply AI-suggested parameter adjustments
    
    Args:
        base_params: Base parameter configuration
        adjustments: AI-suggested adjustments (can be partial)
    
    Returns:
        Adjusted parameters
    """
    adjusted = base_params.copy()
    
    # åº”ç”¨AIå»ºè®®çš„è°ƒæ•´
    for param_name, new_value in adjustments.items():
        if param_name in adjusted:
            adjusted[param_name] = new_value
    
    return adjusted


def translate_insights_to_chinese(insights_en: List[str]) -> List[str]:
    """
    Translate English insights to Chinese for user display
    
    Simple keyword-based translation for common patterns
    """
    insights_zh = []
    
    for insight in insights_en:
        # ç®€å•çš„å…³é”®è¯ç¿»è¯‘ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
        insight_zh = insight
        
        # å¸¸è§æ¨¡å¼ç¿»è¯‘
        replacements = {
            "bullish ratio": "é˜³çº¿æ¯”ä¾‹",
            "best performance": "æ•ˆæœæœ€å¥½",
            "average profit": "å¹³å‡åˆ©æ¶¦",
            "HH-HL structure": "HH-HLç»“æ„",
            "LL-LH structure": "LL-LHç»“æ„",
            "support/resistance": "æ”¯æ’‘/é˜»åŠ›",
            "test": "æµ‹è¯•",
            "times": "æ¬¡",
            "when": "æ—¶",
            "Rank 1 is optimal": "Top 1é…ç½®æœ€ä¼˜",
            "No adjustment needed": "æ— éœ€è°ƒæ•´",
            "Micro-adjust": "å¾®è°ƒ",
            "Market volatility": "å¸‚åœºæ³¢åŠ¨",
            "Risk": "é£é™©"
        }
        
        for en, zh in replacements.items():
            insight_zh = insight_zh.replace(en, zh)
        
        insights_zh.append(f"ğŸ’¡ {insight_zh}")
    
    return insights_zh


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

if __name__ == "__main__":
    print("V8.3.21å›æµ‹ä¼˜åŒ–æ¨¡å—ï¼ˆå«AIè¿­ä»£ï¼‰")
    print("ä½¿ç”¨æ–¹æ³•ï¼šä»ä¸»ç¨‹åºå¯¼å…¥ optimize_params_v8321_lightweight")

