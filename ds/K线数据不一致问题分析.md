# Kçº¿æ•°æ®ä¸ä¸€è‡´é—®é¢˜åˆ†æ

## ğŸ¯ é—®é¢˜ç¡®è®¤

é€šè¿‡æ·±å…¥åˆ†æ `20251114.csv` æ–‡ä»¶ï¼Œå‘ç°äº†**çœŸæ­£çš„é—®é¢˜**ï¼š

**æ‰‹åŠ¨ç”Ÿæˆå’Œç³»ç»Ÿè‡ªåŠ¨è¿è¡Œä¿å­˜çš„Kçº¿æ•°æ®ä¸ä¸€è‡´ï¼Œä¸”å­˜åœ¨é‡å¤ï¼**

---

## ğŸ“Š æ•°æ®å¯¹æ¯”

### åŒä¸€æ—¶é—´ç‚¹ï¼ˆ2000ï¼Œå³20:00ï¼‰çš„BTCæ•°æ®

| å­—æ®µ | ç¬¬1æ¡ï¼ˆæ‰‹åŠ¨ç”Ÿæˆï¼‰ | ç¬¬2æ¡ï¼ˆç³»ç»Ÿè‡ªåŠ¨ï¼‰ | å·®å¼‚å¹…åº¦ |
|------|------------------|------------------|----------|
| **open** | 95737.4 | 95737.4 | 0% âœ… |
| **high** | 95786.2 | **95900.0** | +0.12% âŒ |
| **low** | 95670.6 | **95317.2** | -0.37% âŒ |
| **close** | 95685.4 | **95423.5** | -0.27% âŒ |
| **volume** | 98.39 | **1081.375** | +999% âŒ |

### é‡å¤æ•°æ®ç»Ÿè®¡

```
æ—¶é—´ç‚¹    æ•°æ®æ¡æ•°    çŠ¶æ€
------    --------    ----
0000-1900    7æ¡      æ­£å¸¸ï¼ˆ7ä¸ªå¸ç§ï¼‰
1915         14æ¡     âŒ é‡å¤ï¼
1930         7æ¡      æ­£å¸¸
1945         14æ¡     âŒ é‡å¤ï¼
2000         14æ¡     âŒ é‡å¤ï¼
2015         7æ¡      æ­£å¸¸
2045         7æ¡      æ­£å¸¸
```

---

## ğŸ” é—®é¢˜æ ¹æº

### 1. æ•°æ®è·å–æ—¶æœºä¸åŒ

**æ‰‹åŠ¨ç”Ÿæˆï¼ˆexport_historical_data.pyï¼‰ï¼š**
- ä½¿ç”¨ `exchange.fetch_ohlcv()` è·å–**å†å²å®Œæ•´Kçº¿**
- åœ¨Kçº¿å®Œå…¨å½¢æˆåè·å–
- æ•°æ®ç¨³å®šã€å®Œæ•´

**ç³»ç»Ÿè‡ªåŠ¨ï¼ˆdeepseek_å¤šå¸ç§æ™ºèƒ½ç‰ˆ.pyï¼‰ï¼š**
- ä½¿ç”¨ `exchange.fetch_ohlcv()` è·å–**å®æ—¶Kçº¿**
- å¯èƒ½åœ¨Kçº¿å½¢æˆè¿‡ç¨‹ä¸­è·å–
- æ•°æ®å¯èƒ½ä¸å®Œæ•´æˆ–åç»­æ›´æ–°

### 2. æ•°æ®è¦†ç›–ç­–ç•¥é—®é¢˜

**å½“å‰é€»è¾‘ï¼š**
```python
# export_historical_data.py (æ‰‹åŠ¨ç”Ÿæˆ)
if output_file.exists():
    backup_file = output_dir / f"{date_str}_backup.csv"
    output_file.rename(backup_file)  # å¤‡ä»½æ—§æ–‡ä»¶
# ç„¶åå†™å…¥æ–°æ–‡ä»¶ï¼ˆå®Œå…¨è¦†ç›–ï¼‰

# deepseek_å¤šå¸ç§æ™ºèƒ½ç‰ˆ.py (ç³»ç»Ÿè‡ªåŠ¨)
# ä½¿ç”¨ pandas to_csv(mode='a') è¿½åŠ æ¨¡å¼
# æˆ–è€…ç›´æ¥è¦†ç›–ï¼Œä½†æ—¶æœºä¸å¯¹
```

**é—®é¢˜ï¼š**
- æ‰‹åŠ¨ç”Ÿæˆå…ˆå†™å…¥å®Œæ•´æ•°æ®
- ç³»ç»Ÿè‡ªåŠ¨è¿è¡Œåï¼Œåˆè¿½åŠ äº†å®æ—¶æ•°æ®
- å¯¼è‡´åŒä¸€æ—¶é—´ç‚¹æœ‰ä¸¤æ¡ä¸åŒçš„æ•°æ®

### 3. æ—¶é—´æˆ³å¯¹é½é—®é¢˜

**15åˆ†é’ŸKçº¿çš„æ—¶é—´æˆ³ï¼š**
- æ ‡å‡†æ—¶é—´ï¼š`00:00`, `00:15`, `00:30`, `00:45`, `01:00`...
- æ‰‹åŠ¨ç”Ÿæˆï¼šä¸¥æ ¼å¯¹é½åˆ°15åˆ†é’Ÿæ•´æ•°å€
- ç³»ç»Ÿè‡ªåŠ¨ï¼šå¯èƒ½æœ‰å‡ ç§’çš„åå·®

---

## ğŸš¨ å¯¹å‰ç«¯çš„å½±å“

### ä¸ºä»€ä¹ˆæ‰‹åŠ¨ç”Ÿæˆèƒ½æ˜¾ç¤ºï¼Œè‡ªåŠ¨è¿è¡Œä¸èƒ½ï¼Ÿ

1. **æ•°æ®é‡å¤å¯¼è‡´å‰ç«¯è§£æé”™è¯¯**
   - å‰ç«¯Kçº¿å›¾ç»„ä»¶æœŸæœ›æ¯ä¸ªæ—¶é—´æˆ³åªæœ‰ä¸€æ¡æ•°æ®
   - é‡åˆ°é‡å¤æ—¶é—´æˆ³ï¼Œå¯èƒ½ï¼š
     - åªæ˜¾ç¤ºç¬¬ä¸€æ¡ï¼ˆæ‰‹åŠ¨ç”Ÿæˆçš„å®Œæ•´æ•°æ®ï¼‰ âœ…
     - æˆ–è€…æŠ¥é”™/ä¸æ˜¾ç¤º âŒ

2. **æ•°æ®ä¸å®Œæ•´å¯¼è‡´å›¾è¡¨æ¸²æŸ“å¤±è´¥**
   - ç³»ç»Ÿè‡ªåŠ¨ä¿å­˜çš„å®æ—¶Kçº¿å¯èƒ½ä¸å®Œæ•´
   - `high`ã€`low` ç­‰å­—æ®µå¯èƒ½è¿˜ä¼šå˜åŒ–
   - å‰ç«¯å›¾è¡¨ç»„ä»¶æ£€æµ‹åˆ°æ•°æ®å¼‚å¸¸ï¼Œæ‹’ç»æ¸²æŸ“

3. **æ•°æ®é‡é—®é¢˜**
   - æ‰‹åŠ¨ç”Ÿæˆï¼šå®Œæ•´çš„96æ¡æ•°æ®ï¼ˆ24å°æ—¶ Ã— 4æ¬¡/å°æ—¶ï¼‰
   - ç³»ç»Ÿè‡ªåŠ¨ï¼šåªæœ‰éƒ¨åˆ†æ—¶é—´ç‚¹çš„æ•°æ®
   - å‰ç«¯éœ€è¦æœ€å°‘Næ¡æ•°æ®æ‰èƒ½æ¸²æŸ“Kçº¿å›¾

---

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šä¿®æ”¹ç³»ç»Ÿè‡ªåŠ¨ä¿å­˜é€»è¾‘ï¼ˆæ¨èï¼‰

**ç›®æ ‡ï¼šé¿å…é‡å¤ï¼Œåªä¿å­˜å®Œæ•´Kçº¿**

```python
def save_market_snapshot_v7(market_data_list):
    """ä¿å­˜å¸‚åœºå¿«ç…§ï¼ˆæ¯15åˆ†é’Ÿï¼‰ä¾›å¤ç›˜åˆ†æ"""
    try:
        # ... ç°æœ‰ä»£ç  ...
        
        # ã€æ–°å¢ã€‘æ£€æŸ¥Kçº¿æ˜¯å¦å®Œæ•´
        current_time = datetime.now()
        current_minute = current_time.minute
        
        # åªåœ¨15åˆ†é’Ÿæ•´æ•°å€çš„ç¬¬1åˆ†é’Ÿå†…ä¿å­˜ï¼ˆé¿å…Kçº¿æœªå®Œæˆï¼‰
        # ä¾‹å¦‚ï¼š00:01, 00:16, 00:31, 00:46
        if current_minute % 15 != 1:
            print(f"â° è·³è¿‡ä¿å­˜ï¼šå½“å‰æ—¶é—´ {current_time.strftime('%H:%M')} ä¸æ˜¯Kçº¿å®Œæˆæ—¶åˆ»")
            return
        
        # ã€æ–°å¢ã€‘è¯»å–ç°æœ‰æ–‡ä»¶ï¼Œå»é‡
        if snapshot_file.exists():
            try:
                existing_df = pd.read_csv(snapshot_file)
                # å»é‡ï¼šä¿ç•™æœ€æ–°çš„æ•°æ®
                combined_df = pd.concat([existing_df, new_df])
                combined_df = combined_df.drop_duplicates(
                    subset=['time', 'coin'], 
                    keep='last'  # ä¿ç•™æœ€åä¸€æ¡ï¼ˆæœ€æ–°çš„ï¼‰
                )
                combined_df.to_csv(snapshot_file, index=False)
            except:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œç›´æ¥è¦†ç›–
                new_df.to_csv(snapshot_file, index=False)
        else:
            new_df.to_csv(snapshot_file, index=False)
        
        # ... ç°æœ‰ä»£ç ç»§ç»­ ...
```

### æ–¹æ¡ˆ2ï¼šä¿®æ”¹æ‰‹åŠ¨ç”Ÿæˆé€»è¾‘

**ç›®æ ‡ï¼šä¸è¦†ç›–ç³»ç»Ÿè‡ªåŠ¨ä¿å­˜çš„æ•°æ®**

```python
def export_date(date_str, output_dirs):
    """å¯¼å‡ºæŒ‡å®šæ—¥æœŸçš„CSVåˆ°å¤šä¸ªç›®å½•"""
    # ... ç°æœ‰ä»£ç  ...
    
    for output_dir in output_dirs:
        output_file = output_dir / filename
        
        # ã€ä¿®æ”¹ã€‘ä¸å¤‡ä»½ï¼Œè€Œæ˜¯åˆå¹¶
        if output_file.exists():
            try:
                existing_df = pd.read_csv(output_file)
                new_df = pd.DataFrame(all_rows)
                
                # åˆå¹¶å¹¶å»é‡
                combined_df = pd.concat([existing_df, new_df])
                combined_df = combined_df.drop_duplicates(
                    subset=['time', 'coin'], 
                    keep='last'  # ä¿ç•™æ‰‹åŠ¨ç”Ÿæˆçš„ï¼ˆæ›´å®Œæ•´ï¼‰
                )
                combined_df.to_csv(output_file, index=False)
                print(f"  âœ“ å·²åˆå¹¶: {output_file} ({len(combined_df)} æ¡è®°å½•)")
            except:
                # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œç›´æ¥è¦†ç›–
                with open(output_file, 'w', encoding='utf-8', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_rows)
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥å†™å…¥
            with open(output_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_rows)
```

### æ–¹æ¡ˆ3ï¼šå‰ç«¯å¢åŠ å®¹é”™å¤„ç†

**ç›®æ ‡ï¼šå‰ç«¯èƒ½å¤„ç†é‡å¤æ—¶é—´æˆ³**

```javascript
// å‰ç«¯Kçº¿æ•°æ®å¤„ç†
function processKlineData(rawData) {
    // æŒ‰æ—¶é—´æˆ³åˆ†ç»„
    const groupedByTime = {};
    
    rawData.forEach(item => {
        const time = item.time;
        if (!groupedByTime[time]) {
            groupedByTime[time] = [];
        }
        groupedByTime[time].push(item);
    });
    
    // å¯¹æ¯ä¸ªæ—¶é—´ç‚¹ï¼Œé€‰æ‹©æœ€å®Œæ•´çš„æ•°æ®
    const processedData = [];
    Object.keys(groupedByTime).forEach(time => {
        const items = groupedByTime[time];
        
        if (items.length === 1) {
            processedData.push(items[0]);
        } else {
            // å¤šæ¡æ•°æ®ï¼Œé€‰æ‹©volumeæœ€å¤§çš„ï¼ˆæ›´å®Œæ•´ï¼‰
            const best = items.reduce((prev, curr) => 
                (curr.volume > prev.volume) ? curr : prev
            );
            processedData.push(best);
        }
    });
    
    return processedData;
}
```

---

## ğŸ¯ æ¨èå®æ–½é¡ºåº

### ç¬¬1æ­¥ï¼šç«‹å³ä¿®å¤ï¼ˆå‰ç«¯å®¹é”™ï¼‰

åœ¨å‰ç«¯æ·»åŠ å»é‡é€»è¾‘ï¼Œè®©Kçº¿å›¾èƒ½æ­£å¸¸æ˜¾ç¤ºã€‚

### ç¬¬2æ­¥ï¼šä¿®å¤ç³»ç»Ÿè‡ªåŠ¨ä¿å­˜

ä¿®æ”¹ `save_market_snapshot_v7()` å‡½æ•°ï¼š
1. æ·»åŠ æ—¶é—´æ£€æŸ¥ï¼Œåªåœ¨Kçº¿å®Œæˆåä¿å­˜
2. æ·»åŠ å»é‡é€»è¾‘ï¼Œé¿å…é‡å¤æ•°æ®

### ç¬¬3æ­¥ï¼šä¿®å¤æ‰‹åŠ¨ç”Ÿæˆ

ä¿®æ”¹ `export_historical_data.py`ï¼š
1. æ”¹ä¸ºåˆå¹¶æ¨¡å¼ï¼Œè€Œä¸æ˜¯è¦†ç›–æ¨¡å¼
2. æ·»åŠ å»é‡é€»è¾‘

### ç¬¬4æ­¥ï¼šæ¸…ç†ç°æœ‰æ•°æ®

è¿è¡Œè„šæœ¬æ¸…ç†æ‰€æœ‰é‡å¤æ•°æ®ï¼š

```python
import pandas as pd
from pathlib import Path

def clean_duplicates(file_path):
    """æ¸…ç†CSVæ–‡ä»¶ä¸­çš„é‡å¤æ•°æ®"""
    df = pd.read_csv(file_path)
    
    # å»é‡ï¼šä¿ç•™volumeæœ€å¤§çš„ï¼ˆæ›´å®Œæ•´ï¼‰
    df = df.sort_values('volume', ascending=False)
    df = df.drop_duplicates(subset=['time', 'coin'], keep='first')
    df = df.sort_values(['time', 'coin'])
    
    df.to_csv(file_path, index=False)
    print(f"âœ… å·²æ¸…ç†: {file_path}")

# æ¸…ç†æ‰€æœ‰æ–‡ä»¶
for model in ['qwen', 'deepseek']:
    snapshot_dir = Path(f'/root/10-23-bot/ds/trading_data/{model}/market_snapshots')
    for csv_file in snapshot_dir.glob('*.csv'):
        clean_duplicates(csv_file)
```

---

## ğŸ“ éªŒè¯æ–¹æ³•

### 1. æ£€æŸ¥æ•°æ®æ˜¯å¦å»é‡

```bash
# ç»Ÿè®¡æ¯ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®æ¡æ•°
cut -d',' -f1,2 /root/10-23-bot/ds/trading_data/qwen/market_snapshots/20251114.csv | \
    grep -v "^time" | \
    awk -F',' '{print $1}' | \
    sort | uniq -c

# åº”è¯¥æ¯ä¸ªæ—¶é—´ç‚¹éƒ½æ˜¯7æ¡ï¼ˆ7ä¸ªå¸ç§ï¼‰
```

### 2. æ£€æŸ¥å‰ç«¯æ˜¯å¦æ­£å¸¸æ˜¾ç¤º

è®¿é—®å‰ç«¯é¡µé¢ï¼ŒæŸ¥çœ‹Kçº¿å›¾æ˜¯å¦æ­£å¸¸æ¸²æŸ“ã€‚

### 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§

```bash
# æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„24å°æ—¶æ•°æ®ï¼ˆ96ä¸ªæ—¶é—´ç‚¹ï¼‰
cut -d',' -f1 /root/10-23-bot/ds/trading_data/qwen/market_snapshots/20251114.csv | \
    grep -v "^time" | \
    sort -u | \
    wc -l

# åº”è¯¥æ˜¯96ï¼ˆ24å°æ—¶ Ã— 4æ¬¡/å°æ—¶ï¼‰
```

---

## ğŸ”§ ç›¸å…³æ–‡ä»¶

éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ï¼š
1. `ds/deepseek_å¤šå¸ç§æ™ºèƒ½ç‰ˆ.py` - `save_market_snapshot_v7()` å‡½æ•°
2. `ds/qwen_å¤šå¸ç§æ™ºèƒ½ç‰ˆ.py` - `save_market_snapshot_v7()` å‡½æ•°
3. `ds/export_historical_data.py` - `export_date()` å‡½æ•°
4. å‰ç«¯Kçº¿æ•°æ®å¤„ç†ä»£ç ï¼ˆå¦‚æœæœ‰ï¼‰

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ—¶é—´åŒæ­¥**
   - ç¡®ä¿æœåŠ¡å™¨æ—¶é—´å‡†ç¡®
   - Kçº¿æ—¶é—´æˆ³å¿…é¡»å¯¹é½åˆ°15åˆ†é’Ÿæ•´æ•°å€

2. **æ•°æ®ä¸€è‡´æ€§**
   - åŒä¸€æ—¶é—´ç‚¹åªèƒ½æœ‰ä¸€æ¡æ•°æ®
   - ä¼˜å…ˆä¿ç•™æ›´å®Œæ•´çš„æ•°æ®ï¼ˆvolumeæ›´å¤§ï¼‰

3. **å‘åå…¼å®¹**
   - ä¿®æ”¹åè¦èƒ½å¤„ç†æ—§æ•°æ®
   - æä¾›æ•°æ®æ¸…ç†è„šæœ¬

4. **æ€§èƒ½è€ƒè™‘**
   - å»é‡æ“ä½œå¯èƒ½å½±å“æ€§èƒ½
   - è€ƒè™‘åªåœ¨å¿…è¦æ—¶å»é‡

