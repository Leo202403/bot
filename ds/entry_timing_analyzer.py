"""
ã€V8.3.22ã€‘å¼€ä»“æ—¶æœºåˆ†ææ¨¡å—
ã€V8.3.23ã€‘AIè‡ªä¸»å­¦ä¹ ç‰ˆï¼šä½¿ç”¨AIæ·±åº¦åˆ†æå¹¶ç”Ÿæˆè‹±æ–‡æ´å¯Ÿ
ç‹¬ç«‹æ–‡ä»¶ä¾¿äºç»´æŠ¤å’Œæµ‹è¯•
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import os
from openai import OpenAI


def analyze_entry_timing(yesterday_trades, kline_snapshots, missed_opportunities):
    """
    ã€V8.3.22ã€‘åˆ†æå¼€ä»“æ—¶æœºè´¨é‡
    
    å››ä¸ªç»´åº¦åˆ†æï¼š
    1. è™šå‡ä¿¡å·å¼€ä»“ï¼šå¼€ä»“åå¿«é€Ÿæ­¢æŸï¼Œä¸”å¸‚åœºæœªæŒ‰é¢„æœŸæ–¹å‘èµ°
    2. å»¶è¿Ÿå¼€ä»“ï¼šé”™è¿‡æœ€ä½³å…¥åœºç‚¹ï¼Œå¯¼è‡´R:Ré™ä½æˆ–ç›ˆåˆ©å‡å°‘
    3. è¿‡æ—©å¼€ä»“ï¼šè¶‹åŠ¿æœªç¡®è®¤å°±å…¥åœºï¼Œå¯¼è‡´è¢«æ´—ç›˜æ­¢æŸ
    4. é”™è¿‡æœºä¼šæ·±åº¦åˆ†æï¼šä¸ºä»€ä¹ˆæ²¡å¼€ä»“ï¼ˆå‚æ•°/ä¿¡å·/è¶‹åŠ¿é—®é¢˜ï¼‰
    
    Args:
        yesterday_trades: DataFrame, æ˜¨æ—¥å¼€ä»“çš„äº¤æ˜“
        kline_snapshots: DataFrame, Kçº¿å¿«ç…§æ•°æ®
        missed_opportunities: list, é”™è¿‡çš„æœºä¼šåˆ—è¡¨ï¼ˆæ¥è‡ªanalyze_missed_opportunitiesï¼‰
    
    Returns:
        {
            'entry_stats': {...},  # ç»Ÿè®¡æ•°æ®
            'false_entries': [...],  # è™šå‡ä¿¡å·æ¡ˆä¾‹
            'delayed_entries': [...],  # å»¶è¿Ÿå¼€ä»“æ¡ˆä¾‹
            'premature_entries': [...],  # è¿‡æ—©å¼€ä»“æ¡ˆä¾‹
            'missed_quality_analysis': {...},  # é”™è¿‡æœºä¼šçš„æ·±åº¦åˆ†æ
            'entry_lessons': [...]  # å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®
        }
    """
    
    entry_stats = {
        'total_entries': len(yesterday_trades),
        'false_entries': 0,
        'delayed_entries': 0,
        'premature_entries': 0,
        'optimal_entries': 0
    }
    
    false_entries = []
    delayed_entries = []
    premature_entries = []
    entry_lessons = []
    
    # ===== åˆ†æ1ï¼šè™šå‡ä¿¡å·å¼€ä»“ =====
    for idx, trade in yesterday_trades.iterrows():
        coin = trade.get('å¸ç§', '')
        side = trade.get('æ–¹å‘', '')
        entry_time_str = trade.get('å¼€ä»“æ—¶é—´', '')
        entry_price = trade.get('å¼€ä»“ä»·æ ¼', 0)
        pnl = trade.get('ç›ˆäº', 0)
        exit_reason = trade.get('å¹³ä»“åŸå› ', '')
        
        if not entry_time_str or entry_price == 0:
            continue
        
        try:
            entry_time = pd.to_datetime(entry_time_str)
        except:
            continue
        
        # è·å–è¯¥å¸ç§çš„Kçº¿æ•°æ®
        coin_klines = kline_snapshots[kline_snapshots['coin'] == coin].copy()
        if coin_klines.empty:
            continue
        
        coin_klines['time'] = pd.to_datetime(coin_klines['time'])
        coin_klines = coin_klines.sort_values('time')
        
        # ã€è™šå‡ä¿¡å·åˆ¤æ–­ã€‘ï¼šå¼€ä»“å2å°æ—¶å†…æ­¢æŸï¼Œä¸”åç»­4å°æ—¶æœªå›åˆ°å…¥åœºä»·
        if 'æ­¢æŸ' in exit_reason and pnl < 0:
            # è·å–å¼€ä»“åçš„Kçº¿ï¼ˆ2å°æ—¶çª—å£ç”¨äºæ­¢æŸåˆ¤æ–­ï¼‰
            sl_window_klines = coin_klines[
                (coin_klines['time'] >= entry_time) & 
                (coin_klines['time'] <= entry_time + timedelta(hours=2))
            ]
            
            # è·å–æ­¢æŸåçš„Kçº¿ï¼ˆ4å°æ—¶çª—å£ç”¨äºéªŒè¯æ–¹å‘ï¼‰
            validation_klines = coin_klines[
                (coin_klines['time'] > entry_time + timedelta(hours=2)) & 
                (coin_klines['time'] <= entry_time + timedelta(hours=6))
            ]
            
            if not sl_window_klines.empty and not validation_klines.empty:
                # æ£€æŸ¥æ­¢æŸåå¸‚åœºæ˜¯å¦æŒ‰é¢„æœŸæ–¹å‘èµ°
                if side == 'å¤š':
                    # å¤šå•æ­¢æŸåï¼Œå¦‚æœåç»­ä»·æ ¼ä»æœªä¸Šæ¶¨å›å…¥åœºä»·ï¼Œè¯´æ˜æ˜¯è™šå‡ä¿¡å·
                    max_price_after = validation_klines['high'].max()
                    is_false_signal = max_price_after < entry_price * 0.99  # æœªå›åˆ°å…¥åœºä»·çš„99%
                else:
                    # ç©ºå•æ­¢æŸåï¼Œå¦‚æœåç»­ä»·æ ¼ä»æœªä¸‹è·Œå›å…¥åœºä»·ï¼Œè¯´æ˜æ˜¯è™šå‡ä¿¡å·
                    min_price_after = validation_klines['low'].min()
                    is_false_signal = min_price_after > entry_price * 1.01  # æœªå›åˆ°å…¥åœºä»·çš„101%
                
                if is_false_signal:
                    entry_stats['false_entries'] += 1
                    
                    # åˆ†æä¿¡å·è´¨é‡ï¼ˆä»tradeè®°å½•ä¸­æå–ï¼‰
                    signal_score = trade.get('ä¿¡å·è¯„åˆ†', 0)
                    consensus = trade.get('å…±æŒ¯æŒ‡æ ‡', 0)
                    
                    false_entries.append({
                        'coin': coin,
                        'side': side,
                        'entry_time': entry_time_str,
                        'entry_price': entry_price,
                        'pnl': pnl,
                        'signal_score': signal_score,
                        'consensus': consensus,
                        'issue': f"å¼€ä»“å{len(sl_window_klines)*5}åˆ†é’Ÿæ­¢æŸï¼Œä¸”åç»­{len(validation_klines)*5}åˆ†é’Ÿæœªæ¢å¤",
                        'lesson': f"ä¿¡å·{signal_score}åˆ†+å…±æŒ¯{consensus}ä¸è¶³ä»¥è¿‡æ»¤æ­¤è™šå‡ä¿¡å·"
                    })
        
        # ã€å»¶è¿Ÿå¼€ä»“åˆ¤æ–­ã€‘ï¼šæ£€æŸ¥å…¥åœºå‰æ˜¯å¦æœ‰æ›´å¥½çš„ä»·æ ¼
        entry_window_klines = coin_klines[
            (coin_klines['time'] >= entry_time - timedelta(hours=2)) & 
            (coin_klines['time'] < entry_time)
        ]
        
        if not entry_window_klines.empty:
            if side == 'å¤š':
                # å¤šå•ï¼šå…¥åœºå‰2å°æ—¶çš„æœ€ä½ä»·
                best_entry_price = entry_window_klines['low'].min()
                price_improvement_pct = (entry_price - best_entry_price) / best_entry_price * 100
                
                # å¦‚æœå®é™…å…¥åœºä»·æ¯”æœ€ä½³ä»·é«˜2%ä»¥ä¸Šï¼Œè§†ä¸ºå»¶è¿Ÿ
                if price_improvement_pct > 2.0:
                    entry_stats['delayed_entries'] += 1
                    delayed_entries.append({
                        'coin': coin,
                        'side': side,
                        'entry_time': entry_time_str,
                        'actual_entry': entry_price,
                        'best_entry': best_entry_price,
                        'missed_improvement': price_improvement_pct,
                        'issue': f"é”™è¿‡æ›´ä½{price_improvement_pct:.1f}%çš„å…¥åœºç‚¹",
                        'lesson': f"ä¿¡å·å‡ºç°æ—¶åº”ç«‹å³æ‰§è¡Œï¼Œé¿å…ç­‰å¾…"
                    })
            else:
                # ç©ºå•ï¼šå…¥åœºå‰2å°æ—¶çš„æœ€é«˜ä»·
                best_entry_price = entry_window_klines['high'].max()
                price_improvement_pct = (best_entry_price - entry_price) / entry_price * 100
                
                if price_improvement_pct > 2.0:
                    entry_stats['delayed_entries'] += 1
                    delayed_entries.append({
                        'coin': coin,
                        'side': side,
                        'entry_time': entry_time_str,
                        'actual_entry': entry_price,
                        'best_entry': best_entry_price,
                        'missed_improvement': price_improvement_pct,
                        'issue': f"é”™è¿‡æ›´é«˜{price_improvement_pct:.1f}%çš„å…¥åœºç‚¹",
                        'lesson': f"ä¿¡å·å‡ºç°æ—¶åº”ç«‹å³æ‰§è¡Œï¼Œé¿å…ç­‰å¾…"
                    })
        
        # ã€è¿‡æ—©å¼€ä»“åˆ¤æ–­ã€‘ï¼šå…¥åœºåçŸ­æœŸéœ‡è¡ï¼Œè¢«æ´—ç›˜æ­¢æŸ
        if 'æ­¢æŸ' in exit_reason and pnl < 0:
            # æ£€æŸ¥æ­¢æŸåæ˜¯å¦å¸‚åœºæŒ‰é¢„æœŸæ–¹å‘èµ°äº†
            post_exit_klines = coin_klines[
                (coin_klines['time'] > entry_time) & 
                (coin_klines['time'] <= entry_time + timedelta(hours=6))
            ]
            
            if not post_exit_klines.empty:
                if side == 'å¤š':
                    # å¤šå•æ­¢æŸåï¼Œå¦‚æœåç»­ä¸Šæ¶¨è¶…è¿‡5%ï¼Œè¯´æ˜æ–¹å‘å¯¹ä½†å…¥åœºæ—©äº†
                    max_price_later = post_exit_klines['high'].max()
                    later_rally_pct = (max_price_later - entry_price) / entry_price * 100
                    
                    if later_rally_pct > 5.0:
                        entry_stats['premature_entries'] += 1
                        premature_entries.append({
                            'coin': coin,
                            'side': side,
                            'entry_time': entry_time_str,
                            'entry_price': entry_price,
                            'pnl': pnl,
                            'later_move': later_rally_pct,
                            'issue': f"è¢«æ´—ç›˜æ­¢æŸï¼Œä½†åç»­ä¸Šæ¶¨{later_rally_pct:.1f}%",
                            'lesson': f"åº”ç­‰å¾…å›è°ƒç¡®è®¤æˆ–æ›´æ˜ç¡®çš„çªç ´ä¿¡å·"
                        })
                else:
                    # ç©ºå•æ­¢æŸåï¼Œå¦‚æœåç»­ä¸‹è·Œè¶…è¿‡5%ï¼Œè¯´æ˜æ–¹å‘å¯¹ä½†å…¥åœºæ—©äº†
                    min_price_later = post_exit_klines['low'].min()
                    later_drop_pct = (entry_price - min_price_later) / entry_price * 100
                    
                    if later_drop_pct > 5.0:
                        entry_stats['premature_entries'] += 1
                        premature_entries.append({
                            'coin': coin,
                            'side': side,
                            'entry_time': entry_time_str,
                            'entry_price': entry_price,
                            'pnl': pnl,
                            'later_move': later_drop_pct,
                            'issue': f"è¢«æ´—ç›˜æ­¢æŸï¼Œä½†åç»­ä¸‹è·Œ{later_drop_pct:.1f}%",
                            'lesson': f"åº”ç­‰å¾…å›è°ƒç¡®è®¤æˆ–æ›´æ˜ç¡®çš„çªç ´ä¿¡å·"
                        })
    
    # è®¡ç®—æœ€ä¼˜å…¥åœºæ•°é‡
    entry_stats['optimal_entries'] = entry_stats['total_entries'] - (
        entry_stats['false_entries'] + 
        entry_stats['delayed_entries'] + 
        entry_stats['premature_entries']
    )
    
    # ===== åˆ†æ2ï¼šé”™è¿‡æœºä¼šçš„æ·±åº¦åˆ†ç±» =====
    missed_quality_analysis = analyze_missed_opportunities_deep(missed_opportunities)
    
    # ===== ç”Ÿæˆå¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®® =====
    
    # å»ºè®®1ï¼šè™šå‡ä¿¡å·è¿‡æ»¤
    if entry_stats['false_entries'] > 0:
        false_rate = entry_stats['false_entries'] / entry_stats['total_entries'] * 100
        if false_rate > 30:
            # æå–è™šå‡ä¿¡å·çš„å…±åŒç‰¹å¾
            valid_scores = [e['signal_score'] for e in false_entries if e['signal_score'] > 0]
            valid_consensus = [e['consensus'] for e in false_entries if e['consensus'] > 0]
            
            if valid_scores:
                avg_signal = np.mean(valid_scores)
                entry_lessons.append(
                    f"è™šå‡ä¿¡å·ç‡{false_rate:.0f}%ï¼šæé«˜å…¥åœºé—¨æ§›è‡³ä¿¡å·â‰¥{avg_signal+5:.0f}åˆ†"
                )
            if valid_consensus:
                avg_consensus = np.mean(valid_consensus)
                entry_lessons.append(
                    f"è™šå‡ä¿¡å·ç‡{false_rate:.0f}%ï¼šæé«˜å…±æŒ¯è¦æ±‚è‡³â‰¥{avg_consensus+1:.0f}"
                )
        elif false_rate > 15:
            entry_lessons.append(
                f"è™šå‡ä¿¡å·ç‡{false_rate:.0f}%ï¼šå¢åŠ è¶‹åŠ¿ç¡®è®¤è¦æ±‚ï¼ˆè‡³å°‘2å°æ—¶è¶‹åŠ¿å¹´é¾„ï¼‰"
            )
    
    # å»ºè®®2ï¼šå»¶è¿Ÿå¼€ä»“æ”¹è¿›
    if entry_stats['delayed_entries'] > 0:
        delayed_rate = entry_stats['delayed_entries'] / entry_stats['total_entries'] * 100
        if delayed_rate > 20:
            avg_missed_pct = np.mean([e['missed_improvement'] for e in delayed_entries])
            entry_lessons.append(
                f"å»¶è¿Ÿå¼€ä»“ç‡{delayed_rate:.0f}%ï¼ˆå¹³å‡é”™è¿‡{avg_missed_pct:.1f}%ï¼‰ï¼šä¿¡å·è§¦å‘åç«‹å³æ‰§è¡Œï¼Œé¿å…ç­‰å¾…"
            )
    
    # å»ºè®®3ï¼šè¿‡æ—©å¼€ä»“æ”¹è¿›
    if entry_stats['premature_entries'] > 0:
        premature_rate = entry_stats['premature_entries'] / entry_stats['total_entries'] * 100
        if premature_rate > 20:
            entry_lessons.append(
                f"è¿‡æ—©å¼€ä»“ç‡{premature_rate:.0f}%ï¼šç­‰å¾…å›è°ƒç¡®è®¤æˆ–çªç ´åå›è¸©æ”¯æ’‘"
            )
    
    # å»ºè®®4ï¼šé”™è¿‡æœºä¼šæ”¹è¿›
    if missed_quality_analysis['total_missed'] > 0:
        top_reason = missed_quality_analysis['miss_reasons_distribution'][0] if missed_quality_analysis['miss_reasons_distribution'] else None
        if top_reason:
            reason_type = top_reason['reason']
            reason_pct = top_reason['percentage']
            
            if 'R:R' in reason_type and reason_pct > 40:
                entry_lessons.append(
                    f"é”™è¿‡æœºä¼š{reason_pct:.0f}%å› R:Rè¿‡ä¸¥ï¼šè€ƒè™‘åˆ†çº§R:Rï¼ˆä½é£é™©4:1ï¼Œä¸­é£é™©3:1ï¼‰"
                )
            elif 'è¶‹åŠ¿ä¸ä¸€è‡´' in reason_type and reason_pct > 40:
                entry_lessons.append(
                    f"é”™è¿‡æœºä¼š{reason_pct:.0f}%å› è¶‹åŠ¿ä¸ä¸€è‡´ï¼šå…è®¸2/3è¶‹åŠ¿ä¸€è‡´å³å¯å¼€ä»“"
                )
            elif 'ä¿¡å·' in reason_type and reason_pct > 40:
                entry_lessons.append(
                    f"é”™è¿‡æœºä¼š{reason_pct:.0f}%å› ä¿¡å·è¯„åˆ†ä¸è¶³ï¼šé™ä½ä¿¡å·è¦æ±‚è‡³{top_reason.get('suggested_threshold', 60)}åˆ†"
                )
    
    return {
        'entry_stats': entry_stats,
        'entry_details': yesterday_trades,  # ğŸ”§ V8.3.25.3: æ·»åŠ entry_detailså­—æ®µä¾›AIåˆ†æä½¿ç”¨
        'false_entries': false_entries[:5],  # TOP5
        'delayed_entries': delayed_entries[:5],  # TOP5
        'premature_entries': premature_entries[:5],  # TOP5
        'missed_quality_analysis': missed_quality_analysis,
        'entry_lessons': entry_lessons
    }


def analyze_missed_opportunities_deep(missed_opportunities):
    """
    ã€V8.3.22ã€‘æ·±åº¦åˆ†æé”™è¿‡çš„æœºä¼šï¼Œåˆ†ç±»ç»Ÿè®¡åŸå› 
    
    Args:
        missed_opportunities: list, é”™è¿‡çš„æœºä¼šåˆ—è¡¨
    
    Returns:
        {
            'total_missed': int,
            'miss_reasons_distribution': [...],  # åŸå› åˆ†å¸ƒï¼ˆæŒ‰å æ¯”æ’åºï¼‰
            'high_quality_missed': [...],  # é«˜è´¨é‡é”™è¿‡æœºä¼šï¼ˆåˆ©æ¶¦>10%ï¼‰
            'actionable_insights': [...]  # å¯æ‰§è¡Œçš„æ´å¯Ÿ
        }
    """
    if not missed_opportunities or len(missed_opportunities) == 0:
        return {
            'total_missed': 0,
            'miss_reasons_distribution': [],
            'high_quality_missed': [],
            'actionable_insights': []
        }
    
    # ç»Ÿè®¡é”™è¿‡åŸå› 
    reason_counts = {}
    high_quality_missed = []
    
    for opp in missed_opportunities:
        reason = opp.get('reason', 'unknown')
        profit = opp.get('potential_profit_pct', 0)
        
        # åˆ†ç±»åŸå› ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if 'R:R' in reason or 'ç›ˆäºæ¯”' in reason:
            reason_category = 'R:Rè¿‡ä¸¥'
        elif 'è¶‹åŠ¿ä¸ä¸€è‡´' in reason or 'è¶‹åŠ¿' in reason:
            reason_category = 'è¶‹åŠ¿ä¸ä¸€è‡´'
        elif 'ä¿¡å·' in reason or 'è¯„åˆ†' in reason:
            reason_category = 'ä¿¡å·è¯„åˆ†ä¸è¶³'
        elif 'å…±æŒ¯' in reason:
            reason_category = 'å…±æŒ¯è¦æ±‚è¿‡é«˜'
        else:
            reason_category = 'å…¶ä»–'
        
        reason_counts[reason_category] = reason_counts.get(reason_category, 0) + 1
        
        # è¯†åˆ«é«˜è´¨é‡é”™è¿‡ï¼ˆåˆ©æ¶¦>10%ï¼‰
        if profit > 10:
            high_quality_missed.append({
                'coin': opp['trend']['coin'],
                'type': opp['trend']['type'],
                'profit': profit,
                'reason': reason_category
            })
    
    # è®¡ç®—åŸå› åˆ†å¸ƒ
    total = len(missed_opportunities)
    miss_reasons_distribution = [
        {
            'reason': reason,
            'count': count,
            'percentage': count / total * 100,
            'suggested_threshold': get_suggested_threshold(reason, count, total)
        }
        for reason, count in sorted(reason_counts.items(), key=lambda x: x[1], reverse=True)
    ]
    
    # ç”Ÿæˆå¯æ‰§è¡Œæ´å¯Ÿ
    actionable_insights = []
    for reason_item in miss_reasons_distribution[:3]:  # TOP3åŸå› 
        if reason_item['percentage'] > 30:  # å æ¯”è¶…è¿‡30%æ‰å€¼å¾—å…³æ³¨
            actionable_insights.append(
                f"TOPåŸå› ï¼š{reason_item['reason']}å {reason_item['percentage']:.0f}% "
                f"â†’ å»ºè®®è°ƒæ•´é˜ˆå€¼è‡³{reason_item['suggested_threshold']}"
            )
    
    return {
        'total_missed': total,
        'miss_reasons_distribution': miss_reasons_distribution,
        'high_quality_missed': high_quality_missed[:5],  # TOP5
        'actionable_insights': actionable_insights
    }


def get_suggested_threshold(reason_type, count, total):
    """
    ã€V8.3.22ã€‘æ ¹æ®é”™è¿‡åŸå› æ¨èæ–°é˜ˆå€¼
    
    åŸºäºç»Ÿè®¡å­¦åŸç†ï¼šå¦‚æœæŸä¸ªåŸå› å¯¼è‡´>30%çš„æœºä¼šè¢«é”™è¿‡ï¼Œéœ€è¦æ”¾å®½æ¡ä»¶
    """
    if 'R:R' in reason_type:
        return "3.5:1 (ä»4.9é™ä½)"
    elif 'ä¿¡å·è¯„åˆ†' in reason_type:
        return "60åˆ† (ä»65é™ä½)"
    elif 'å…±æŒ¯' in reason_type:
        return "2ä¸ª (ä»3é™ä½)"
    elif 'è¶‹åŠ¿' in reason_type:
        return "2/3è¶‹åŠ¿ä¸€è‡´å³å¯"
    else:
        return "éœ€äººå·¥review"


def generate_ai_entry_insights(entry_analysis, exit_analysis, market_context=None, ai_decisions=None):
    """
    ã€V8.3.23ã€‘ä½¿ç”¨AIæ·±åº¦åˆ†æå¼€ä»“è´¨é‡å¹¶ç”Ÿæˆè‹±æ–‡æ´å¯Ÿ
    ã€V8.3.24ã€‘å¢å¼ºï¼šåŒ…å«AIåŸå§‹å†³ç­–ç†ç”±çš„è‡ªæˆ‘åæ€
    
    Args:
        entry_analysis: dict, å¼€ä»“åˆ†æç»“æœï¼ˆæ¥è‡ªanalyze_entry_timingï¼‰
        exit_analysis: dict, å¹³ä»“åˆ†æç»“æœï¼ˆæ¥è‡ªanalyze_exit_timingï¼‰
        market_context: dict, å¸‚åœºç¯å¢ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        ai_decisions: list, AIå†å²å†³ç­–è®°å½•ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
    
    Returns:
        {
            'diagnosis': str,  # é—®é¢˜è¯Šæ–­ï¼ˆè‹±æ–‡ï¼‰
            'root_causes': [str],  # æ ¹æœ¬åŸå› åˆ—è¡¨
            'recommendations': [  # å…·ä½“å»ºè®®
                {
                    'issue': str,  # é—®é¢˜
                    'action': str,  # è¡ŒåŠ¨
                    'threshold': str,  # å…·ä½“é˜ˆå€¼
                    'expected_impact': str  # é¢„æœŸæ•ˆæœ
                }
            ],
            'learning_insights': [str],  # å¯ä¼ é€’ç»™å®æ—¶AIçš„å…³é”®æ´å¯Ÿ
            'generated_at': str
        }
    """
    try:
        # ğŸ”§ V8.3.25.4: è‡ªåŠ¨æ£€æµ‹APIå¯†é’¥å’Œbase_urlï¼ˆæ”¯æŒQwenå’ŒDeepSeekï¼‰
        deepseek_key = os.getenv('DEEPSEEK_API_KEY')
        qwen_key = os.getenv('QWEN_API_KEY')
        
        if deepseek_key:
            api_key = deepseek_key.strip()
            base_url = "https://api.deepseek.com"
            model_type = "DeepSeek"
            model_name = "deepseek-chat"  # ğŸ”§ V8.5.2.4.28: ä½¿ç”¨deepseek-chatä»£æ›¿reasonerï¼ˆreasonerå¯èƒ½è¿”å›ç©ºå“åº”ï¼‰
        elif qwen_key:
            api_key = qwen_key.strip()
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            model_type = "Qwen"
            model_name = "qwen3-max"  # ğŸ”§ V8.3.25.5: ä½¿ç”¨Qwençš„æœ€å¼ºæ¨¡å‹
        else:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼šè¯·è®¾ç½® DEEPSEEK_API_KEY æˆ– QWEN_API_KEY ç¯å¢ƒå˜é‡")
        
        print(f"[AI Entry Analysis] ä½¿ç”¨{model_type} API ({model_name})è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # å‹ç¼©æ•°æ®ï¼ˆé¿å…è¶…é•¿promptï¼‰
        entry_stats = entry_analysis['entry_stats']
        
        # æ„å»ºç²¾ç®€çš„æ¡ˆä¾‹æ‘˜è¦
        false_signals_summary = []
        for entry in entry_analysis.get('false_entries', [])[:3]:
            false_signals_summary.append({
                'coin': entry['coin'],
                'side': entry.get('side', 'N/A'),  # ğŸ”§ V8.3.25.19: V2å¯èƒ½æ²¡æœ‰sideå­—æ®µ
                'signal_score': entry.get('signal_score', 0),
                'consensus': entry.get('consensus', 0),
                'issue': entry.get('issue', entry.get('reason', 'N/A'))  # ğŸ”§ V8.3.25.19: å…¼å®¹reasonå­—æ®µ
            })
        
        delayed_entries_summary = []
        for entry in entry_analysis.get('delayed_entries', [])[:3]:
            delayed_entries_summary.append({
                'coin': entry['coin'],
                'missed_improvement_pct': entry['missed_improvement']
            })
        
        premature_entries_summary = []
        for entry in entry_analysis.get('premature_entries', [])[:3]:
            premature_entries_summary.append({
                'coin': entry['coin'],
                'later_move_pct': entry['later_move']
            })
        
        # æ„å»ºexit statsæ‘˜è¦
        exit_stats_summary = {
            'sl_rate': 0,
            'premature_exits': 0,
            'avg_missed_profit': 0
        }
        if exit_analysis:
            exit_stats = exit_analysis.get('exit_stats', {})
            total_exits = max(exit_stats.get('total_exits', 1), 1)
            exit_stats_summary = {
                'sl_rate': exit_stats.get('sl_exits', 0) / total_exits * 100,
                'premature_exits': exit_stats.get('premature_exits', 0),
                'avg_missed_profit': exit_stats.get('avg_missed_profit_pct', 0)
            }
        
        # ğŸ†• V8.3.24: æå–AIå†³ç­–ç†ç”±ï¼ˆç”¨äºè‡ªæˆ‘åæ€ï¼‰
        # ğŸ”§ V8.3.25: å¢å¼º - ä¸ºæ¯ç¬”äº¤æ˜“åŒ¹é…å¯¹åº”çš„AIå†³ç­–ï¼ˆæ—¶é—´çª—å£Â±5åˆ†é’Ÿï¼‰
        from datetime import datetime, timedelta
        
        def find_ai_decision_for_trade(trade_time_str, coin, ai_decisions):
            """ä¸ºäº¤æ˜“åŒ¹é…AIå†³ç­–ï¼ˆå®¹é”™è·³è¿‡ï¼‰"""
            if not trade_time_str or not ai_decisions:
                return None
            
            try:
                # è§£æäº¤æ˜“æ—¶é—´
                trade_time = datetime.strptime(trade_time_str, '%Y-%m-%d %H:%M:%S')
                
                # åœ¨Â±5åˆ†é’Ÿçª—å£å†…æŸ¥æ‰¾
                for decision in ai_decisions:
                    decision_time_str = decision.get('timestamp', '')
                    if not decision_time_str:
                        continue
                    
                    try:
                        decision_time = datetime.strptime(decision_time_str, '%Y-%m-%d %H:%M:%S')
                        time_diff = abs((trade_time - decision_time).total_seconds())
                        
                        # æ—¶é—´çª—å£ï¼š5åˆ†é’Ÿ = 300ç§’
                        if time_diff <= 300:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¥å¸ç§çš„å¼€ä»“action
                            for action in decision.get('actions', []):
                                if coin in action.get('symbol', '') and 'OPEN' in action.get('action', ''):
                                    return {
                                        'timestamp': decision_time_str,
                                        'thinking': decision.get('æ€è€ƒè¿‡ç¨‹', '')[:150],
                                        'action_reason': action.get('reason', '')[:100],
                                        'time_diff_seconds': int(time_diff)
                                    }
                    except:
                        continue
            except:
                pass
            
            return None
        
        # ä¸ºæ¯ç¬”äº¤æ˜“åŒ¹é…AIå†³ç­–
        ai_reasoning_samples = []
        matched_count = 0
        
        # ğŸ”§ V8.3.25.8: å…¼å®¹V2åˆ†ææ¨¡å—ï¼ˆæ²¡æœ‰entry_detailsï¼Œåªæœ‰entry_table_dataï¼‰
        if 'entry_details' not in entry_analysis:
            print(f"  â„¹ï¸  Entry analysis from V2 module (no entry_details), using simplified AI reflection")
            # V2æ¨¡å—è¿”å›çš„æ•°æ®ç»“æ„ä¸åŒï¼Œæˆ‘ä»¬è·³è¿‡è¯¦ç»†çš„tradeåŒ¹é…ï¼Œåªä½¿ç”¨æœ€è¿‘çš„AIå†³ç­–
            if ai_decisions and len(ai_decisions) > 0:
                for decision in ai_decisions[-10:]:  # ä½¿ç”¨æœ€è¿‘10æ¡å†³ç­–
                    ai_reasoning_samples.append({
                        'timestamp': decision.get('timestamp', ''),
                        'thinking': decision.get('æ€è€ƒè¿‡ç¨‹', '')[:150],
                        'actions': [
                            {
                                'coin': a.get('symbol', '').split('/')[0] if '/' in a.get('symbol', '') else a.get('symbol', ''),
                                'action': a.get('action', ''),
                                'reason': a.get('reason', '')[:100]
                            }
                            for a in decision.get('actions', [])[:2]
                        ]
                    })
                print(f"  âœ“ ä½¿ç”¨äº†{len(ai_reasoning_samples)}æ¡æœ€è¿‘AIå†³ç­–ç”¨äºè‡ªæˆ‘åæ€")
        elif ai_decisions and len(ai_decisions) > 0:
            # æ—§çš„é€»è¾‘ï¼šéå†å¼€ä»“äº¤æ˜“ï¼ŒåŒ¹é…AIå†³ç­–
            for _, trade in entry_analysis['entry_details'].iterrows():
                coin = trade.get('coin', '')
                open_time = trade.get('å¼€ä»“æ—¶é—´', '')
                
                ai_decision = find_ai_decision_for_trade(open_time, coin, ai_decisions)
                if ai_decision:
                    ai_reasoning_samples.append({
                        'coin': coin,
                        'trade_time': open_time,
                        **ai_decision
                    })
                    matched_count += 1
            
            # å¦‚æœåŒ¹é…æ•°å°‘äº5æ¡ï¼Œè¡¥å……å…¶ä»–å†³ç­–ï¼ˆä¿è¯æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼‰
            if len(ai_reasoning_samples) < 5 and len(ai_decisions) > 0:
                for decision in ai_decisions[-5:]:
                    if len(ai_reasoning_samples) >= 5:
                        break
                    
                    # é¿å…é‡å¤
                    if decision.get('timestamp') not in [r['timestamp'] for r in ai_reasoning_samples]:
                        ai_reasoning_samples.append({
                            'timestamp': decision.get('timestamp', ''),
                            'thinking': decision.get('æ€è€ƒè¿‡ç¨‹', '')[:150],
                            'actions': [
                                {
                                    'coin': a.get('symbol', '').split('/')[0],
                                    'action': a.get('action', ''),
                                    'reason': a.get('reason', '')[:100]
                                }
                                for a in decision.get('actions', [])[:2]
                            ]
                        })
            
            print(f"  âœ“ åŒ¹é…äº†{matched_count}ç¬”äº¤æ˜“çš„AIå†³ç­–ï¼ˆÂ±5åˆ†é’Ÿçª—å£ï¼‰")
        
        # ğŸ†• V8.3.25: ä¸ºé”™è¿‡çš„æœºä¼šä¹ŸåŒ¹é…AIå†³ç­–ï¼ˆåˆ†æ"ä¸ºä»€ä¹ˆæ²¡å¼€ä»“"ï¼‰
        missed_with_ai_decisions = []
        if ai_decisions and 'missed_opportunities' in locals():
            for opp in missed_opportunities[:10]:  # åªåˆ†æTOP10é”™è¿‡çš„æœºä¼š
                opp_time = opp.get('time', '')
                opp_coin = opp.get('coin', '')
                
                if not opp_time or not opp_coin:
                    continue
                
                # åœ¨Â±5åˆ†é’Ÿçª—å£å†…æŸ¥æ‰¾AIå†³ç­–
                try:
                    opp_dt = datetime.strptime(opp_time, '%Y-%m-%d %H:%M:%S')
                    
                    for decision in ai_decisions:
                        decision_time_str = decision.get('timestamp', '')
                        if not decision_time_str:
                            continue
                        
                        try:
                            decision_dt = datetime.strptime(decision_time_str, '%Y-%m-%d %H:%M:%S')
                            time_diff = abs((opp_dt - decision_dt).total_seconds())
                            
                            # æ—¶é—´çª—å£ï¼š5åˆ†é’Ÿ
                            if time_diff <= 300:
                                # æ£€æŸ¥AIæ˜¯å¦è€ƒè™‘è¿‡è¿™ä¸ªå¸ç§
                                ai_mentioned_coin = False
                                for action in decision.get('actions', []):
                                    if opp_coin in action.get('symbol', ''):
                                        ai_mentioned_coin = True
                                        break
                                
                                # å¦‚æœAIæ²¡æè¿™ä¸ªå¸ï¼Œè¯´æ˜å¯èƒ½è¢«è¿‡æ»¤äº†
                                # ğŸ†• V8.5.1.8: æ·»åŠ ä¿¡å·åˆ†æ•°å’Œå…±æŒ¯æ•°ï¼Œå¸®åŠ©AIåˆ†æè¿‡æ»¤é€»è¾‘æ˜¯å¦åˆç†
                                missed_with_ai_decisions.append({
                                    'coin': opp_coin,
                                    'missed_time': opp_time,
                                    'missed_reason': opp.get('reason', 'unknown'),
                                    'profit_potential': opp.get('profit', 0),
                                    'signal_score': opp.get('signal_score', 0),  # ğŸ†• V8.5.1.8
                                    'consensus': opp.get('consensus', 0),        # ğŸ†• V8.5.1.8
                                    'ai_decision_time': decision_time_str,
                                    'ai_considered': ai_mentioned_coin,
                                    'ai_thinking': decision.get('æ€è€ƒè¿‡ç¨‹', '')[:100],
                                    'time_diff_seconds': int(time_diff)
                                })
                                break
                        except:
                            continue
                except:
                    continue
            
            if missed_with_ai_decisions:
                print(f"  âœ“ åŒ¹é…äº†{len(missed_with_ai_decisions)}ä¸ªé”™è¿‡æœºä¼šçš„AIå†³ç­–")
        
        # æ„å»ºæ•°æ®åŒ…
        # ğŸ”§ V8.3.25.8: å…¼å®¹V2æ¨¡å—ï¼ˆä½¿ç”¨total_opportunitiesè€Œä¸æ˜¯total_entriesï¼‰
        total_count = entry_stats.get('total_entries', entry_stats.get('total_opportunities', entry_stats.get('ai_opened', 1)))
        false_entries = entry_stats.get('false_entries', 0)
        delayed_entries = entry_stats.get('delayed_entries', entry_stats.get('timing_issues', 0))  # V2ç”¨timing_issues
        premature_entries = entry_stats.get('premature_entries', 0)
        optimal_entries = entry_stats.get('optimal_entries', entry_stats.get('correct_entries', 0))  # V2ç”¨correct_entries
        
        # ğŸ†• V8.5.1.8: è®¡ç®—ä¿¡å·è´¨é‡ç»Ÿè®¡ï¼ˆå¯¹æ¯”è™šå‡ä¿¡å· vs æ­£ç¡®ä¿¡å·ï¼‰
        signal_quality_comparison = {}
        try:
            if false_signals_summary:
                false_scores = [f.get('signal_score', 0) for f in false_signals_summary if f.get('signal_score', 0) > 0]
                false_consensus = [f.get('consensus', 0) for f in false_signals_summary if f.get('consensus', 0) > 0]
                
                if false_scores and false_consensus:
                    signal_quality_comparison['false_signals'] = {
                        'avg_signal_score': np.mean(false_scores),
                        'avg_consensus': np.mean(false_consensus),
                        'count': len(false_scores)
                    }
            
            # è·å–æ­£ç¡®å¼€ä»“çš„æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            correct_entries_data = entry_analysis.get('correct_entries', [])
            if correct_entries_data:
                correct_scores = [c.get('signal_score', 0) for c in correct_entries_data if c.get('signal_score', 0) > 0]
                correct_consensus = [c.get('consensus', 0) for c in correct_entries_data if c.get('consensus', 0) > 0]
                
                if correct_scores and correct_consensus:
                    signal_quality_comparison['correct_entries'] = {
                        'avg_signal_score': np.mean(correct_scores),
                        'avg_consensus': np.mean(correct_consensus),
                        'count': len(correct_scores)
                    }
        except Exception as e:
            print(f"  âš ï¸ ä¿¡å·è´¨é‡ç»Ÿè®¡å¤±è´¥: {e}")
        
        analysis_data = {
            'entry_quality': {
                'total_entries': total_count,
                'false_signal_rate': false_entries / max(total_count, 1) * 100,
                'delayed_rate': delayed_entries / max(total_count, 1) * 100,
                'premature_rate': premature_entries / max(total_count, 1) * 100,
                'optimal_rate': optimal_entries / max(total_count, 1) * 100
            },
            'false_signals': false_signals_summary,
            'delayed_entries': delayed_entries_summary,
            'premature_entries': premature_entries_summary,
            'exit_quality': exit_stats_summary,
            'market_context': market_context or {},
            'ai_reasoning_samples': ai_reasoning_samples,  # ğŸ†• AIå†³ç­–ç†ç”±
            'missed_with_ai': missed_with_ai_decisions,  # ğŸ†• V8.3.25: é”™è¿‡æœºä¼šçš„AIå†³ç­–
            'signal_quality_comparison': signal_quality_comparison  # ğŸ†• V8.5.1.8: ä¿¡å·è´¨é‡å¯¹æ¯”
        }
        
        # æ„å»ºAI promptï¼ˆçº¯è‹±æ–‡ + è‡ªæˆ‘åæ€ï¼‰
        ai_reasoning_note = ""
        if ai_reasoning_samples:
            ai_reasoning_note = f"""

# ğŸ§  AI Self-Reflection Context
The AI system has been making decisions with the following reasoning patterns:
```json
{json.dumps(ai_reasoning_samples[-3:], indent=2)}
```

**CRITICAL**: Analyze these reasoning patterns against the actual results. 
- What logical flaws led to false signals?
- What assumptions were wrong?
- What market conditions were misinterpreted?

Provide specific critique of the AI's decision-making process."""
        
        # ğŸ†• V8.3.25: æ·»åŠ é”™è¿‡æœºä¼šçš„AIåæ€
        # ğŸ”§ V8.5.1.8: å¢å¼ºpromptï¼Œå¼•å¯¼AIåˆ†æä¿¡å·è´¨é‡ä¸è¿‡æ»¤é€»è¾‘çš„å…³ç³»
        missed_ai_note = ""
        if missed_with_ai_decisions:
            # ğŸ†• V8.5.1.8: è®¡ç®—é”™è¿‡æœºä¼šçš„å¹³å‡ä¿¡å·è´¨é‡
            avg_missed_score = np.mean([m.get('signal_score', 0) for m in missed_with_ai_decisions]) if missed_with_ai_decisions else 0
            avg_missed_consensus = np.mean([m.get('consensus', 0) for m in missed_with_ai_decisions]) if missed_with_ai_decisions else 0
            high_quality_missed = [m for m in missed_with_ai_decisions if m.get('signal_score', 0) >= 75 and m.get('consensus', 0) >= 3]
            
            missed_ai_note = f"""

# ğŸ” Missed Opportunities with AI Decision Context
Analysis of why the AI didn't enter these profitable opportunities:
```json
{json.dumps(missed_with_ai_decisions[:5], indent=2)}
```

**CRITICAL SIGNAL QUALITY ANALYSIS** (V8.5.1.8 Enhanced):
- Average signal_score of missed opportunities: {avg_missed_score:.1f}
- Average consensus of missed opportunities: {avg_missed_consensus:.1f}
- High-quality missed (score>=75, consensus>=3): {len(high_quality_missed)} opportunities

**KEY QUESTIONS**:
1. **Signal Quality Check**: 
   - If avg signal_score > 70 and consensus > 2.5, these were HIGH-QUALITY signals
   - Why were they filtered? Were thresholds too strict?
   
2. **AI Consideration**:
   - Did the AI consider this coin at that time? (ai_considered field)
   - If yes, why did it decide NOT to open? Was the logic correct or overly conservative?
   - If no, why was this coin filtered out? Was it a systematic blind spot?
   
3. **Profit vs Quality**:
   - Given the profit_potential (actual profit if entered) vs signal_score/consensus
   - Were the filtering criteria appropriate?

**ACTIONABLE OUTPUT REQUIRED**:
- If high-quality signals (score>=75, consensus>=3) were missed â†’ RECOMMEND lowering thresholds
- If low-quality signals were correctly filtered â†’ VALIDATE current thresholds
- Specify exact threshold adjustments: "min_signal_score >= X", "min_consensus >= Y"

Provide specific, quantified insights on whether the AI's filtering logic needs adjustment."""

        # ğŸ†• V8.5.1.8: æ„å»ºä¿¡å·è´¨é‡å¯¹æ¯”æ‘˜è¦ï¼ˆæ˜¾å¼å±•ç¤ºç»™AIï¼‰
        quality_comparison_note = ""
        if signal_quality_comparison:
            false_sig = signal_quality_comparison.get('false_signals', {})
            correct_sig = signal_quality_comparison.get('correct_entries', {})
            
            if false_sig and correct_sig:
                score_diff = correct_sig['avg_signal_score'] - false_sig['avg_signal_score']
                consensus_diff = correct_sig['avg_consensus'] - false_sig['avg_consensus']
                recommended_score = correct_sig['avg_signal_score'] * 0.95  # å»ºè®®é˜ˆå€¼ï¼šæ­£ç¡®ä¿¡å·å¹³å‡å€¼çš„95%
                recommended_consensus = max(2, correct_sig['avg_consensus'] * 0.9)  # å»ºè®®é˜ˆå€¼ï¼šæ­£ç¡®ä¿¡å·å¹³å‡å€¼çš„90%ï¼Œæœ€å°‘2
                
                quality_comparison_note = f"""

# ğŸ“Š Signal Quality Comparison (V8.5.1.8 Enhanced)
**FALSE SIGNALS** ({false_sig['count']} samples):
- Average signal_score: {false_sig['avg_signal_score']:.1f}
- Average consensus: {false_sig['avg_consensus']:.1f}

**CORRECT ENTRIES** ({correct_sig['count']} samples):
- Average signal_score: {correct_sig['avg_signal_score']:.1f}
- Average consensus: {correct_sig['avg_consensus']:.1f}

**QUALITY GAP**:
- Signal score difference: {score_diff:+.1f} points (correct entries are {score_diff:.1f} points higher)
- Consensus difference: {consensus_diff:+.1f} indicators (correct entries have {consensus_diff:.1f} more)

**RECOMMENDED THRESHOLDS** (based on data):
- min_signal_score >= {recommended_score:.0f} (95% of correct entries' average)
- min_consensus >= {recommended_consensus:.0f} (90% of correct entries' average)

â†’ Use these statistics to calibrate your threshold recommendations!"""
            elif false_sig:
                quality_comparison_note = f"""

# ğŸ“Š Signal Quality Analysis (V8.5.1.8)
**FALSE SIGNALS** ({false_sig['count']} samples):
- Average signal_score: {false_sig['avg_signal_score']:.1f}
- Average consensus: {false_sig['avg_consensus']:.1f}

â†’ Consider raising thresholds above these averages to filter false signals."""
        
        prompt = f"""You are an expert quantitative trading analyst performing AI self-reflection analysis. 

# Entry Quality Data
```json
{json.dumps(analysis_data, indent=2)}
```
{quality_comparison_note}
{ai_reasoning_note}
{missed_ai_note}

# Your Task
Perform deep self-critical analysis:
1. **Review AI's past reasoning** (if provided above) and identify logical errors
2. **Analyze entry quality results** to find patterns of failure
3. **Connect the dots**: How did flawed reasoning lead to poor results?
4. **Generate corrective insights** that address root causes in decision logic

# Requirements
1. **Diagnosis**: Identify the core issue in AI's decision-making process (1-2 sentences)
2. **Root Causes**: List 2-3 fundamental logical flaws (with specific examples from reasoning if available)
3. **Recommendations**: Provide 3-5 actionable recommendations:
   - Specific threshold adjustments (with numbers)
   - Decision logic corrections (e.g., "Don't trust MACD golden cross when RSI>70")
   - Expected impact (quantified if possible)
   - Implementation priority (High/Medium/Low)
   - **CRITICAL**: For threshold field, use EXACT format: "parameter_name >= value" or "parameter_name: value"
     Examples: "min_risk_reward >= 3.0", "min_signal_score >= 70", "atr_stop_multiplier: 1.8"
4. **Learning Insights**: Generate 3-5 key learnings that critique and correct AI's reasoning patterns

# Output Format (JSON)
{{
  "diagnosis": "Brief summary of the main issue",
  "root_causes": [
    "Cause 1: ...",
    "Cause 2: ..."
  ],
  "recommendations": [
    {{
      "issue": "What problem this addresses",
      "action": "Specific action to take",
      "threshold": "min_risk_reward >= 3.0",
      "expected_impact": "e.g., Reduce false signal rate by 10-15%",
      "priority": "High/Medium/Low"
    }}
  ],
  "learning_insights": [
    "Insight 1: Pattern observed...",
    "Insight 2: Condition discovered..."
  ]
}}

# Important
- Focus on patterns, not individual cases
- Provide specific numbers for thresholds
- **threshold field MUST use format: "parameter_name >= value" or "parameter_name: value"**
- Ensure insights are actionable for AI
- Output must be valid JSON
"""
        
        # è°ƒç”¨AIåˆ†æ
        print(f"[AI Entry Analysis] Calling {model_type} AI ({model_name}) for deep insights...")
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional quant trading analyst specialized in entry timing optimization. Always output valid JSON."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,  # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
            max_tokens=4000  # ğŸ”§ V8.3.25.21: DeepSeek reasoneréœ€è¦æ›´å¤štokensï¼ˆæ€è€ƒè¿‡ç¨‹é•¿ï¼‰
        )
        
        # è§£æAIå“åº”
        ai_content = response.choices[0].message.content.strip()
        
        # ğŸ”§ V8.3.25.26: è°ƒè¯•è¾“å‡ºåŸå§‹å“åº”
        print(f"[AI Entry Analysis] ğŸ“ åŸå§‹å“åº”é•¿åº¦: {len(ai_content)} å­—ç¬¦")
        print(f"[AI Entry Analysis] ğŸ“ å“åº”å‰500å­—ç¬¦:\n{ai_content[:500]}")
        print(f"[AI Entry Analysis] ğŸ“ å“åº”å200å­—ç¬¦:\n{ai_content[-200:]}")
        
        # æå–JSONï¼ˆå¯èƒ½è¢«markdownåŒ…è£¹ï¼‰
        if '```json' in ai_content:
            ai_content = ai_content.split('```json')[1].split('```')[0].strip()
        elif '```' in ai_content:
            ai_content = ai_content.split('```')[1].split('```')[0].strip()
        
        # ğŸ”§ V8.3.25.14: å¢å¼ºJSONè§£æ - å¤„ç†DeepSeekçš„æ ¼å¼é—®é¢˜
        try:
            ai_insights = json.loads(ai_content)
        except json.JSONDecodeError as json_err:
            print(f"[AI Entry Analysis] âš ï¸ JSONè§£æå¤±è´¥: {json_err}")
            print(f"[AI Entry Analysis] ğŸ“ å¤±è´¥çš„JSONå†…å®¹:\n{ai_content}")
            print(f"[AI Entry Analysis] ğŸ”§ å°è¯•ä¿®å¤JSONæ ¼å¼...")
            
            # ğŸ”§ V8.3.25.26: å¢å¼ºJSONä¿®å¤é€»è¾‘
            try:
                # 1. ç§»é™¤å¯èƒ½çš„ä¸å®Œæ•´JSONå°¾éƒ¨
                if ai_content.rstrip().endswith(','):
                    ai_content = ai_content.rstrip()[:-1]
                
                # 2. æŸ¥æ‰¾ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}ï¼Œæå–å®Œæ•´JSONå¯¹è±¡
                first_brace = ai_content.find('{')
                last_brace = ai_content.rfind('}')
                
                if first_brace >= 0 and last_brace > first_brace:
                    ai_content = ai_content[first_brace:last_brace+1]
                    print(f"[AI Entry Analysis] ğŸ”§ æå–JSONç‰‡æ®µ: {first_brace}åˆ°{last_brace}")
                
                # 3. ä¿®å¤å¸¸è§çš„å­—ç¬¦ä¸²æˆªæ–­é—®é¢˜
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„å¼•å·ï¼ˆåœ¨æœ€åä¸€ä¸ªå€¼å¤„ï¼‰
                open_quotes = ai_content.count('"')
                if open_quotes % 2 != 0:
                    # å¥‡æ•°ä¸ªå¼•å·ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å­—æ®µ
                    # å›æº¯åˆ°ä¸Šä¸€ä¸ªå®Œæ•´çš„å­—æ®µ
                    patterns = [
                        r',\s*"[^"]+"\s*:\s*"[^"]*$',  # æœªé—­åˆçš„å­—ç¬¦ä¸²å€¼
                        r',\s*"[^"]+"\s*:\s*\[[^\]]*$',  # æœªé—­åˆçš„æ•°ç»„
                    ]
                    import re
                    for pattern in patterns:
                        match = re.search(pattern, ai_content)
                        if match:
                            ai_content = ai_content[:match.start()] + '}'
                            print(f"[AI Entry Analysis] ğŸ”§ ä¿®å¤æœªé—­åˆå­—æ®µï¼Œæˆªå–åˆ°ä½ç½®{match.start()}")
                            break
                
                # 4. å†æ¬¡å°è¯•è§£æ
                ai_insights = json.loads(ai_content)
                print(f"[AI Entry Analysis] âœ… JSONä¿®å¤æˆåŠŸ")
            except Exception as fix_err:
                print(f"[AI Entry Analysis] âŒ JSONä¿®å¤å¤±è´¥: {fix_err}")
                print(f"[AI Entry Analysis] ğŸ’¡ å»ºè®®: å¢åŠ max_tokensæˆ–ä½¿ç”¨éreasoneræ¨¡å‹")
                return {
                    'diagnosis': 'JSONè§£æå¤±è´¥ï¼Œæ— æ³•æå–AIæ´å¯Ÿ',
                    'learning_insights': [],
                    'key_recommendations': [],
                    'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
        
        # æ·»åŠ æ—¶é—´æˆ³
        ai_insights['generated_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        ai_insights['tokens_used'] = response.usage.total_tokens
        ai_insights['cost_usd'] = response.usage.total_tokens * 0.0000014  # DeepSeek pricing
        
        print(f"[AI Entry Analysis] âœ“ Generated {len(ai_insights.get('recommendations', []))} recommendations")
        print(f"[AI Entry Analysis] âœ“ Tokens: {ai_insights['tokens_used']}, Cost: ${ai_insights['cost_usd']:.6f}")
        
        return ai_insights
        
    except Exception as e:
        print(f"[AI Entry Analysis] âš ï¸ Failed: {e}")
        import traceback
        traceback.print_exc()
        
        # é™çº§è¿”å›ç©ºç»“æ„
        from datetime import datetime as dt_fallback  # ğŸ”§ V8.3.25.19: é¿å…UnboundLocalError
        return {
            'diagnosis': f"AI analysis failed: {str(e)}",
            'root_causes': [],
            'recommendations': [],
            'learning_insights': [],
            'generated_at': dt_fallback.now().strftime('%Y-%m-%d %H:%M:%S'),
            'error': str(e)
        }


def generate_ai_exit_insights(exit_analysis, entry_analysis=None, market_context=None, ai_decisions=None):
    """
    ã€V8.3.23ã€‘ä½¿ç”¨AIæ·±åº¦åˆ†æå¹³ä»“è´¨é‡å¹¶ç”Ÿæˆè‹±æ–‡æ´å¯Ÿ
    ã€V8.3.24ã€‘å¢å¼ºï¼šåŒ…å«AIåŸå§‹å†³ç­–ç†ç”±çš„è‡ªæˆ‘åæ€
    
    Args:
        exit_analysis: dict, å¹³ä»“åˆ†æç»“æœï¼ˆæ¥è‡ªanalyze_exit_timingï¼‰
        entry_analysis: dict, å¼€ä»“åˆ†æç»“æœï¼ˆå¯é€‰ï¼Œç”¨äºå…³è”åˆ†æï¼‰
        market_context: dict, å¸‚åœºç¯å¢ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        ai_decisions: list, AIå†å²å†³ç­–è®°å½•ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
    
    Returns: åŒgenerate_ai_entry_insightsæ ¼å¼
    """
    try:
        # ğŸ”§ V8.3.25.4: è‡ªåŠ¨æ£€æµ‹APIå¯†é’¥å’Œbase_urlï¼ˆæ”¯æŒQwenå’ŒDeepSeekï¼‰
        deepseek_key = os.getenv('DEEPSEEK_API_KEY')
        qwen_key = os.getenv('QWEN_API_KEY')
        
        if deepseek_key:
            api_key = deepseek_key.strip()
            base_url = "https://api.deepseek.com"
            model_type = "DeepSeek"
            model_name = "deepseek-chat"  # ğŸ”§ V8.5.2.4.28: ä½¿ç”¨deepseek-chatä»£æ›¿reasonerï¼ˆreasonerå¯èƒ½è¿”å›ç©ºå“åº”ï¼‰
        elif qwen_key:
            api_key = qwen_key.strip()
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            model_type = "Qwen"
            model_name = "qwen3-max"  # ğŸ”§ V8.3.25.5: ä½¿ç”¨Qwençš„æœ€å¼ºæ¨¡å‹
        else:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼šè¯·è®¾ç½® DEEPSEEK_API_KEY æˆ– QWEN_API_KEY ç¯å¢ƒå˜é‡")
        
        print(f"[AI Exit Analysis] ä½¿ç”¨{model_type} API ({model_name})è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # å‹ç¼©æ•°æ®
        exit_stats = exit_analysis['exit_stats']
        
        # æ„å»ºæ¡ˆä¾‹æ‘˜è¦
        premature_exits_summary = []
        for trade in exit_analysis.get('suboptimal_exits', [])[:5]:
            premature_exits_summary.append({
                'coin': trade.get('coin', 'N/A'),
                'side': trade.get('side', 'N/A'),
                'missed_profit_pct': trade.get('missed_profit_pct', 0),
                'exit_type': trade.get('exit_type', 'N/A'),
                'evaluation': trade.get('evaluation', 'N/A')
            })
        
        good_exits_summary = []
        for trade in exit_analysis.get('good_exits', [])[:3]:
            good_exits_summary.append({
                'coin': trade.get('coin', 'N/A'),
                'evaluation': trade.get('evaluation', 'N/A')
            })
        
        # ğŸ†• V8.3.24: æå–AIå†³ç­–ç†ç”±ï¼ˆå¹³ä»“ç›¸å…³ï¼‰
        ai_exit_reasoning = []
        if ai_decisions and len(ai_decisions) > 0:
            recent_decisions = ai_decisions[-10:] if len(ai_decisions) > 10 else ai_decisions
            for decision in recent_decisions:
                # åªæå–CLOSE_LONG/CLOSE_SHORTç›¸å…³çš„å†³ç­–
                exit_actions = [
                    action for action in decision.get('actions', [])
                    if 'CLOSE' in action.get('action', '')
                ]
                if exit_actions:
                    ai_exit_reasoning.append({
                        'timestamp': decision.get('timestamp', ''),
                        'thinking': decision.get('æ€è€ƒè¿‡ç¨‹', '')[:150],
                        'exit_actions': [
                            {
                                'coin': a.get('symbol', '').split('/')[0],
                                'action': a.get('action', ''),
                                'reason': a.get('reason', '')[:100]
                            }
                            for a in exit_actions[:2]
                        ]
                    })
        
        # æ„å»ºæ•°æ®åŒ…
        analysis_data = {
            'exit_quality': {
                'total_exits': exit_stats['total_exits'],
                'tp_rate': exit_stats['tp_exits'] / max(exit_stats['total_exits'], 1) * 100,
                'sl_rate': exit_stats['sl_exits'] / max(exit_stats['total_exits'], 1) * 100,
                'premature_exits': exit_stats['premature_exits'],
                'optimal_exits': exit_stats['optimal_exits'],
                'avg_missed_profit_pct': exit_stats.get('avg_missed_profit_pct', 0)
            },
            'premature_cases': premature_exits_summary,
            'good_cases': good_exits_summary,
            'exit_lessons': exit_analysis.get('exit_lessons', []),
            'ai_exit_reasoning': ai_exit_reasoning  # ğŸ†• AIå¹³ä»“å†³ç­–ç†ç”±
        }
        
        # æ„å»ºAI promptï¼ˆåŒ…å«è‡ªæˆ‘åæ€ï¼‰
        ai_reasoning_note = ""
        if ai_exit_reasoning:
            ai_reasoning_note = f"""

# ğŸ§  AI Self-Reflection: Exit Decision Reasoning
The AI's past exit decisions and reasoning:
```json
{json.dumps(ai_exit_reasoning[-3:], indent=2)}
```

**CRITICAL**: Analyze if the AI's exit reasoning was sound:
- Did it exit too early based on fear rather than data?
- Did it ignore bullish continuation signals?
- Were take-profit targets too conservative?
"""

        prompt = f"""You are an expert quantitative trading analyst performing AI self-reflection on exit timing.

# Exit Quality Data
```json
{json.dumps(analysis_data, indent=2)}
```

# Your Task
Perform deep analysis and generate insights that can be used by the AI trading system to improve future exit decisions.

# Requirements
1. **Diagnosis**: Identify the core issue with exit timing
2. **Root Causes**: List 2-3 fundamental reasons for suboptimal exits
3. **Recommendations**: Provide 3-5 actionable recommendations for:
   - Take-profit strategy optimization
   - Stop-loss adjustment
   - Trailing stop implementation
   - Risk-reward ratio refinement
   - **CRITICAL**: For threshold field, use EXACT format: "parameter_name >= value" or "parameter_name: value"
     Examples: "atr_tp_multiplier: 3.5", "min_risk_reward >= 2.5", "trailing_stop_pct: 0.8"
4. **Learning Insights**: Generate 3-5 key learnings for real-time AI reference

# Output Format (JSON)
{{
  "diagnosis": "Brief summary of exit timing issues",
  "root_causes": ["Cause 1", "Cause 2"],
  "recommendations": [
    {{
      "issue": "Problem",
      "action": "Solution",
      "threshold": "atr_tp_multiplier: 3.5",
      "expected_impact": "Quantified improvement",
      "priority": "High/Medium/Low"
    }}
  ],
  "learning_insights": [
    "Insight 1: Exit pattern observed...",
    "Insight 2: Condition for optimal exit..."
  ]
}}

# Important
- Focus on systematic patterns
- **threshold field MUST use format: "parameter_name >= value" or "parameter_name: value"**
- Provide specific threshold adjustments
- Ensure insights are immediately actionable
- Output valid JSON only
"""
        
        # è°ƒç”¨AIåˆ†æ
        print(f"[AI Exit Analysis] Calling {model_type} AI ({model_name}) for deep insights...")
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional quant trading analyst specialized in exit timing optimization. Always output valid JSON."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,
            max_tokens=4000  # ğŸ”§ V8.3.25.21: DeepSeek reasoneréœ€è¦æ›´å¤štokensï¼ˆæ€è€ƒè¿‡ç¨‹é•¿ï¼‰
        )
        
        # è§£æAIå“åº”
        ai_content = response.choices[0].message.content.strip()
        
        # æå–JSON
        if '```json' in ai_content:
            ai_content = ai_content.split('```json')[1].split('```')[0].strip()
        elif '```' in ai_content:
            ai_content = ai_content.split('```')[1].split('```')[0].strip()
        
        # ğŸ”§ V8.3.25.14: å¢å¼ºJSONè§£æ - å¤„ç†DeepSeekçš„æ ¼å¼é—®é¢˜
        try:
            ai_insights = json.loads(ai_content)
        except json.JSONDecodeError as json_err:
            print(f"[AI Exit Analysis] âš ï¸ JSONè§£æå¤±è´¥: {json_err}")
            print(f"[AI Exit Analysis] ğŸ”§ å°è¯•ä¿®å¤JSONæ ¼å¼...")
            
            # å°è¯•ä¿®å¤å¸¸è§é—®é¢˜ï¼šæœªé—­åˆçš„å­—ç¬¦ä¸²
            try:
                # ç§»é™¤å¯èƒ½çš„ä¸å®Œæ•´JSONå°¾éƒ¨
                if ai_content.rstrip().endswith(','):
                    ai_content = ai_content.rstrip()[:-1]
                
                # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
                last_brace = ai_content.rfind('}')
                if last_brace > 0:
                    ai_content = ai_content[:last_brace+1]
                
                ai_insights = json.loads(ai_content)
                print(f"[AI Exit Analysis] âœ… JSONä¿®å¤æˆåŠŸ")
            except:
                print(f"[AI Exit Analysis] âŒ JSONä¿®å¤å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                return {
                    'diagnosis': 'JSONè§£æå¤±è´¥ï¼Œæ— æ³•æå–AIæ´å¯Ÿ',
                    'learning_insights': [],
                    'key_recommendations': [],
                    'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
        
        # æ·»åŠ å…ƒæ•°æ®
        ai_insights['generated_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        ai_insights['tokens_used'] = response.usage.total_tokens
        ai_insights['cost_usd'] = response.usage.total_tokens * 0.0000014
        
        print(f"[AI Exit Analysis] âœ“ Generated {len(ai_insights.get('recommendations', []))} recommendations")
        print(f"[AI Exit Analysis] âœ“ Tokens: {ai_insights['tokens_used']}, Cost: ${ai_insights['cost_usd']:.6f}")
        
        return ai_insights
        
    except Exception as e:
        print(f"[AI Exit Analysis] âš ï¸ Failed: {e}")
        import traceback
        traceback.print_exc()
        
        from datetime import datetime as dt_fallback  # ğŸ”§ V8.3.25.19: é¿å…UnboundLocalError
        return {
            'diagnosis': f"AI analysis failed: {str(e)}",
            'root_causes': [],
            'recommendations': [],
            'learning_insights': [],
            'generated_at': dt_fallback.now().strftime('%Y-%m-%d %H:%M:%S'),
            'error': str(e)
        }

